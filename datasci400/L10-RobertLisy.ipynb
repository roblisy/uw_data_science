{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10\n",
    "\n",
    "Instructions\n",
    "Find an example of crazy, surprising, or misleading statistics in data and write a brief op-ed on the situation.\n",
    "\n",
    "Include the URL for your source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "I think one of my favorite examples of a complex set of statistics is around US household income. I picked this example because there's pieces of the data which can support any conclusion you like.\n",
    "\n",
    "For people who believe that US household wealth is increasing, the US median household income has risen steadily since 1970, from $ 50,000 / household to $ 75,000 / household in 2018. That's an increase of 50%!\n",
    "\n",
    "For people who want to believe that Americans are getting poorer, the aggregate share of wealth in the US which flows to lower income households has been steadily decreasing. This means that the poorest Americans are seeing less of the benefits from increasing economic growth.\n",
    "\n",
    "For folks who think the rich win outsized gains, the top 5% of the wealithiest households in the US have seen the highest rates of annual growth in their incomes.\n",
    "\n",
    "There's so much to slice under these numbers that there's no one answer, which is kind of beautiful. This could be an example of cherry picking in stats, where you chose the numbers (among many) which best support your argument.\n",
    "\n",
    "[Source](https://www.pewsocialtrends.org/2020/01/09/trends-in-income-and-wealth-inequality/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "Here's another great example of a stats scandal - it's called p-hacking. Most frequentist statistics based studies have set a \"p-value\" threshold at 95%. This is what people talk about as a \"95% confidence interval\". It's the belief that there's only a 5% chance that an experiment's results could have happened by accident.\n",
    "\n",
    "The weird thing though is that 95% confidence doesn't mean anything - it's not a magic number. In fact it's just a historical tradition, one that some fields (physics for instance) have determined is inadequate.\n",
    "\n",
    "The interesting thing about a 95% CI though is that it means there's a 5% (or 1 in 20) chance you ARE getting the results by accident. So if you want to have a \"statistically significant\" result, you could just keep repeating the experiment about 20 times.\n",
    "\n",
    "That's effectively what Brian Wansink, the head of Cornell University's food lab, was caught doing in 2018. His group had collected a large portion of data, and then was just re-querying it for \"statistically significant\" findings. [A nice article about this practice is here](https://www.npr.org/sections/thesalt/2018/09/26/651849441/cornell-food-researchers-downfall-raises-larger-questions-for-science)\n",
    "\n",
    "\n",
    "\n",
    "[Physics folks use way higher p-values](https://www2.mpia-hd.mpg.de/homes/calj/higgsJuly2012.html#:~:text=What%20p%2Dvalues%20mean,predicted%20for%20the%20Higgs%20boson.&text=Using%20a%20two%2Dsided%20test,or%201%20in%201.7%20million.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
