{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "Version: Jun-2019\n",
    "<p>Look for the <b>9 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - Introduction to Hypothesis Testing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This note book introduces you to the concepts of classical hypothesis testing. Specifically, this notebook covers the following cases:\n",
    "\n",
    "- Two sample tests for continuous variables\n",
    "- Two sample tests for categorical or count data\n",
    "- Test for distributions\n",
    "\n",
    "At the conclusion of this lesson you should be able apply basic classical hypothesis tests to some common situations. \n",
    "\n",
    "\n",
    "### Examples of Hypothesis Tests\n",
    "\n",
    "What is a practical scenario of using hypothesis tests methods?\n",
    "\n",
    "- **Identify a hypothesis that can be tested:** Using a larger logo on our web site will drive significantly more traffic.\n",
    "- **Select a criteria to evaluate the hypothesis:** If our sample has a probability of $>= 90\\%$ chance that the increase could be caused by randomness alone, we will reject the null hypothesis that the logo does not matter.\n",
    "- **Select a random sample from the population:** Randomly assign a cookie to new site users that tells the server to show A or B website.\n",
    "- **Calculate a statistic** to compare observations to what we expect to observe.\n",
    "\n",
    "### Hypothesis Testing Steps\n",
    "\n",
    "Let's illustrate the use of an hypothesis test with an example.\n",
    "\n",
    "- We first state our population assumptions in the null hypothesis: $H_0$.\n",
    "- We state our new alternative hypothesis as an alternative to the null: $H_a$.\n",
    "- The null + alternative should make up all possible outcomes and be mutually exclusive.\n",
    "  - $H_0 =$ The old website drives an equal amount of traffic as the new one.\n",
    "  - $H_a =$ The old website drives less traffic than the new one.\n",
    "- Decide on a significance level (probability cutoff): 0.9, 0.95, and 0.99 are common (problem specific).\n",
    "\n",
    "Based on our findings we can only do two things:\n",
    "- **Reject the null-hypothesis:** Since the alternative covers all other possibilities, we can say we accept the alternative hypothesis.\n",
    "- **Fail to reject the null hypothesis:**  We accept the null hypothesis because we have already believed our null hypothesis from the start. We can fail for two reasons:\n",
    "  - The alternative hypothesis was false to begin with.\n",
    "  - We did not collect enough evidence for the **size of the effect**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example\n",
    "\n",
    "Let's try a simple example. We will start by computing the one-sided tail probability of a normal distribution. In this case we will look at the probability of a the interval from $20.1262055$ to near infinity for a normal distribution with mean of 15 and standard deviation of 4. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One tailed probability\n",
    "def prob_normal(a, b, mean=0, sd=1):\n",
    "    import scipy.stats as ss\n",
    "    return(ss.norm.cdf(b, mean, sd) - ss.norm.cdf(a, mean, sd))\n",
    "prob_normal(20.1262055, 100000000, 15, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the other side\n",
    "prob_normal(-1000000000, 20.1262055, 15, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of the normal distribution having a value greater than $20.1262055$ is 0.10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the probability of the entire distribution is 1.0 by executing the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_normal(-100000000, 100000000, 15, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 1\n",
    "Find the probability of a standard normal distribution in the following intervals:\n",
    "- -1 to 1\n",
    "- -2 to 2\n",
    "- -3 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the approximate probabilities? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Does the Cutoff Mean?\n",
    "\n",
    "- We know that the average time a user spends on a page has a mean of 15 seconds and a s.d. of 4 seconds.\n",
    "- If we assume normality, how do we test if a change to the page has a higher view time?\n",
    "- For example, an event in the blue region will have a 10% chance or less of occurring.\n",
    "\n",
    "![cutoff](https://library.startlearninglabs.uw.edu/DATASCI410/img/cutoff.jpg)\n",
    "\n",
    "**Reject the Null Hypothesis with $90\\%$ confidence** if the mean of the time spent on the new web page is in the blue region. \n",
    "\n",
    "The code in the cell below computes the $90\\%$ cutoff for a normal distribution with a mean of 15 and a standard deviation of 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_stat(alpha, mean=0, sd=1, one_tailed=True):\n",
    "    from scipy.stats import norm\n",
    "    if((alpha > 0.0) & (alpha < 1.0)):\n",
    "        if(one_tailed):\n",
    "            return(norm.ppf(1.0 - alpha, loc=mean, scale=sd))\n",
    "        else:\n",
    "            return(norm.ppf(1.0 - alpha/2.0, loc=mean, scale=sd))\n",
    "    else:   \n",
    "        print('alpha must be btween 0 and 1')\n",
    "cutoff_stat(0.1, mean = 15.0, sd = 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can be $90\\%$ confident that a value greater than 20.1 is drawn from another population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many Tails Do You Have?\n",
    "\n",
    "The above is a one-sided hypothesis test. But, **be careful** as there are other alternatives! Could the new web site be worse? In fact, there are **three possibilities** for which tail(s) used to evaluate the test, as shown in the figure.\n",
    "\n",
    "<img alt=\"3 Tails graph\" style=\"height: 300px;\" src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/tails2.jpg\" />\n",
    "\n",
    "The accept-rejection criteria for the null hypothesis is different in each case.\n",
    "\n",
    "- One-tail test with value $\\gt$ the cutoff\n",
    "- One-tail test with value $\\lt$ the cutoff\n",
    "- Two-tail test with value $\\lt\\ -cutoff/2$ or $\\gt\\ cutoff/2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is  the P-value?\n",
    "\n",
    "Results of hypothesis tests are often reported as a p-value. You must be careful to correctly interpret the p-value. \n",
    "\n",
    "- In technical terms, a p-value is the probability of obtaining an effect **at least as extreme** as the one in your sample data, assuming the null hypothesis is true.\n",
    "For example, for a vaccine study with a p-value of 0.04, you’d obtain the observed difference or more in 4% of studies due to random sampling error.\n",
    "**P values address only one question: how likely are your data, assuming a true null hypothesis?** \n",
    "- **P value does not measure support for the alternative hypothesis!**\n",
    "\n",
    "\n",
    "### Misuse of P-values\n",
    "\n",
    "But, watch out! There are many ways to misinterpret or misuse p-values.\n",
    "\n",
    "1. The most common mistake: interpreting a p-value as the probability of mistakenly rejecting a true null hypothesis (a **type I error**).\n",
    "  - P-values calculations assume the null hypothesis is true for the population and the difference in the sample is entirely from random chance. **P-values can not tell you the probability that the null is true or false!** \n",
    "  - For the vaccine study, correct and incorrect way to interpret a p-value of 0.04:\n",
    "    * Correct: If vaccine has no effect, in 4% of studies **the observed difference or more arises solely from random sampling error.**\n",
    "    * Incorrect: By rejecting the null hypothesis, there’s a 4% chance of Type 1 error.\n",
    "2. The second common mistake: you can deduce the probability that the alternative hypothesis is correct (e.g. $1.0 - p$, or a **type II error**).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Example: t-test\n",
    "\n",
    "Student's t-test is one of the oldest and most widely used classical hypothesis test. Student’s t-test tests a hypothesis about the difference of two data samples:\n",
    "- Tests whether a population mean has a specified value\n",
    "- Tests the difference between two means (equal, unknown variances)\n",
    "- Tests a paired-response difference from zero, e.g., a before/after drug treatment on patients\n",
    "- Tests whether the slope of a line is not zero\n",
    "- Tests the importance of variables (covered later in class)\n",
    "- Use ‘Welch’s T-test’ for testing the difference between two means (unknown variances, potentially different)\n",
    "- **Picking the different tests changes test’s results.**\n",
    "- The more assumptions we make, the easier it is to tell the difference between populations.\n",
    "\n",
    "### Who Was Student?\n",
    "\n",
    "<img title=\"William Sealy Gosset in 1908\" alt=\"Gosset 1908\" style=\"float: left; height: 200px; margin-right: 10px;\" src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Gosset_1908.jpg\" /> \n",
    "Student was a pseudonym for William Sealy Gosset. Gosset worked for Guinness Company which forbade employees from publishing work. Gosset published the theory of the t-test in 1908 under his pseudonym.  \n",
    "\n",
    "<img title=\"Plague at site of Gosset's home\" alt=\"Gosset house plaque\" style=\" height: 200px;\" src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/gossett.jpg\" />\n",
    "\n",
    "### Why the T-distribution?\n",
    "\n",
    "The difference in means between two normal distributions with unknown variance follows a t-distribution. The t-distribution has many nice properties including:\n",
    "\n",
    "- The t-distribution is the natural distribution for tests on means.\n",
    "- The t-distribution has heavier tails than the normal and relaxed assumptions on the differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A First T-test Example\n",
    "\n",
    "As a first example, compare the means of two normal distributions. Execute the code in the cell below to compute samples from two normal distributions with slightly different means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy.random as nr\n",
    "nr.seed(seed=233423)\n",
    "pop_A = norm.rvs(loc=150, scale=7, size=25)\n",
    "pop_B = norm.rvs(loc=153, scale=4, size=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below creates histograms of the two populations along with a line for the mean of each population. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def hist_plot(vec, bins):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statistics import mean\n",
    "    plt.hist(vec, bins = bins)\n",
    "    plt.axvline(mean(vec), color = 'red')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Value')\n",
    "    \n",
    "def plot_pop(a, b, cols=['pop_A', 'pop_B'], nbins = 20):\n",
    "    import matplotlib.pyplot as plt\n",
    "    minx = min([min(a), min(b)])\n",
    "    maxx = max([max(a), max(b)])\n",
    "    stepx = (maxx - minx)/(nbins + 1)\n",
    "    bins = [minx + i * stepx for i in range(nbins + 1)]\n",
    "    plt.subplot(2, 1, 1) # The first plot\n",
    "    hist_plot(a, bins)\n",
    "    plt.title('Histograme of ' + cols[0] + ' and ' + cols[1])\n",
    "    plt.subplot(2, 1, 2) # The next plot\n",
    "    hist_plot(b, bins)\n",
    "    plt.show()\n",
    "\n",
    "plot_pop(pop_A, pop_B)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values of the distributions of these populations overlap quite a bit and the means are close. The question is, are these differences significant?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compute the two-sided t-test to determine if the difference of means is significant. A number of summary statistics are computed and printed for the test. \n",
    "\n",
    "The two-sided t-test is used to determine we can reject the<br>\n",
    "**null hypothesis that the difference of means is not significant**.<br> You can execute the t-test and see the results by executing the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(a, b, alpha, alternative='two-sided'):\n",
    "    from scipy import stats\n",
    "    import scipy.stats as ss\n",
    "    import pandas as pd\n",
    "    import statsmodels.stats.weightstats as ws\n",
    "    \n",
    "    diff = a.mean() - b.mean()\n",
    "\n",
    "    res = ss.ttest_ind(a, b, equal_var=False)\n",
    "      \n",
    "    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n",
    "    confint = means.tconfint_diff(alpha=alpha, alternative=alternative, usevar='unequal') \n",
    "    degfree = means.dof_satt()\n",
    "\n",
    "    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI', 'High95CI']\n",
    "    return pd.Series([degfree, diff, res[0], res[1], confint[0], confint[1]], index = index)   \n",
    "   \n",
    "\n",
    "test = t_test(pop_A, pop_B, 0.05)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine these statistics noticing the following:\n",
    "1. Difference in means is small.\n",
    "2. The t-statistic is small and the p-value is large. Therefore, there is a high chance that the difference in means is from random variation alone. \n",
    "3. The 95% confidence interval straddles 0.  \n",
    "\n",
    "Based on these statistics we cannot reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below makes a similar plot to the ones you have already created, but with the the upper and lower confidence bounds shown as dashed lines. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_mean_ci(vec, t_test, bins):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statistics import mean\n",
    "    upper = mean(vec) + t_test[5] - t_test[1]\n",
    "    lower = mean(vec) + t_test[4] - t_test[1]\n",
    "    plt.hist(vec, bins = bins)\n",
    "    plt.axvline(mean(vec), color = 'red')\n",
    "    plt.axvline(upper, color = 'red', linestyle='--')\n",
    "    plt.axvline(lower, color = 'red', linestyle='--')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Value')\n",
    "\n",
    "def plot_t(a, b, t_test, cols=['pop_A', 'pop_B'], nbins = 20):\n",
    "    import matplotlib.pyplot as plt\n",
    "    minx = min([min(a), min(b)])\n",
    "    maxx = max([max(a), max(b)])\n",
    "    stepx = (maxx - minx)/(nbins + 1)\n",
    "    bins = [minx + i * stepx for i in range(nbins + 1)]\n",
    "    plt.subplot(2, 1, 1) # The first plot\n",
    "    hist_mean_ci(a, t_test, bins)\n",
    "    plt.title('Histograme of ' + cols[0] + ' and ' + cols[1])\n",
    "    plt.subplot(2, 1, 2) # The next plot\n",
    "    hist_plot(b, bins)\n",
    "    plt.show()\n",
    "    \n",
    "plot_t(pop_A, pop_B, test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the mean of the population in the lower plot is within the confidence interval of the difference of means of the populations. This confirms that we cannot reject the null hypothesis that there is no significant difference in these means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 2\n",
    "In the previous example the difference of means was not significant. Now, you will repeat this analysis but with a slightly greater difference in means. Create two populations with means (`loc`) of `150` and `155`. Determine if this difference in means is significant using the `t_test` and `plot_t` functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_C = norm.rvs(loc=???, scale=7, size=25) # replace the ???\n",
    "pop_D = norm.rvs(loc=???, scale=4, size=35) # replace the ???\n",
    "\n",
    "test2 = t_test(pop_C, pop_D, 0.05) # fill in parameters\n",
    "print(test2)\n",
    "plot_t(pop_C, pop_D, test2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine these results and answer the following questions:\n",
    "1. Based on the t-statistic, p-value, and confidence interval, can you reject the null hypothesis? \n",
    "2. Does the mean of the second population fall within the confidence interval of the difference of means? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power of Tests\n",
    "\n",
    "The **power of a test** is formally defined as:\n",
    "\n",
    "$$power = P(reject\\ H_0| when\\ H_a\\ is\\ true)$$\n",
    "\n",
    "In plain language, the power of a test is the probability of getting a positive result when the null hypothesis is not true. Conversely, a test with insufficient power will not detect a real effect. Clearly, we want the most powerful test we can find for the situation. \n",
    "\n",
    "Computing test power can be a bit complex, and analytical solutions can be difficult or impossible. Often, a simulation is used to compute power. \n",
    "\n",
    "Let's look at the example of computation power for the two sample t-tests for the difference of means. The power of this test depends on the several parameters:\n",
    "\n",
    "- The number of samples\n",
    "- The anticipated difference in the population means, which we call the **effect**\n",
    "- The significance level of the test\n",
    "- The type of test\n",
    "\n",
    "When running a power test, you can ask several questions that will assist you in designing an experiment. Usually, you will determine how big a sample you need to have good chance of rejecting the null hypothesis. You can also determine how big an effect needs to be given a fixed sample size (all the samples you have or can afford) to have a good chance of rejecting the null hypothesis. \n",
    "\n",
    "The Python `statsmodels` package provides power calculations for a limited set of hypothesis tests. We can use these capabilities to examine the power.\n",
    "\n",
    "The code in the cell below does the following:\n",
    "\n",
    "- Creates a sequence of effect sizes\n",
    "- Computes a vector of power values for the effect size\n",
    "- Plots the effect size vs. power\n",
    "\n",
    "Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.stats.power as smsp\n",
    "import numpy as np\n",
    "nr.seed(seed=23344)\n",
    "diffs = np.arange(start = 0.0, stop = 1.5, step = .015)\n",
    "powers = [smsp.tt_ind_solve_power(effect_size = x, nobs1 = 25, \n",
    "           alpha = 0.05, power = None, ratio = 1.0, alternative = 'two-sided') \n",
    "          for x in diffs]\n",
    "\n",
    "def plot_power(x, y, xlabel, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(x, y, color = 'red', linewidth = 2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Power')\n",
    "    plt.show()\n",
    "plot_power(diffs, powers, xlabel = 'Difference of means', title = 'Power vs. difference of means')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results and notice how the power of the t-test rapidly increases as the difference in means increases. At a relatively small difference in means the power of the test is approaching 1.0, the maximum possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 3\n",
    "In the code cell below, compute and plot 100 values of test power for significance levels in the range $\\{0.001, 0.25\\}$. Set `n = 25`, `d = 1.0`, for the number of samples and the difference in means.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphas = np.arange(start = ???, stop = ???, step = 0.0025) # replace the ???\n",
    "powers = [smsp.tt_ind_solve_power(effect_size = ???, nobs1 = ???, # replace the ???\n",
    "           alpha = x, power = None, ratio = 1.0, alternative = 'two-sided') \n",
    "          for x in alphas]\n",
    "plot_power(alphas, powers, xlabel = 'Significance level', title = 'Power vs. significance level') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the plot you have created. \n",
    "At a significance level of 0.20 is the power of the t-test approaching 1.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Your Turn 4\n",
    "In the code cell below, compute and plot 100 values of test power for sample size in the range $\\{1, 100\\}$. Set `d = 1.0`, and `sig.level = 0.05`, for the difference in means and the significance level.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = np.arange(start = ???, stop = ???, step = 1.0) # replace the ???\n",
    "powers = [smsp.tt_ind_solve_power(effect_size = ???, nobs1 = x, # replace the ???\n",
    "           alpha = ???, power = None, ratio = 1.0, alternative = 'two-sided') \n",
    "          for x in nobs]\n",
    "plot_power(nobs, powers, xlabel = 'Number of observations', title = 'Number of observations') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the plot you have created. \n",
    "The power of the t-test at 20 samples is approximately what value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests on Galton's Height Data (1886)\n",
    "\n",
    "Francis Galton published his seminal paper on the theory of regression in 1886. This paper has been enormously influential in statistics, as the linear regression model is still used today, although in a different form from Galton's approach. \n",
    "\n",
    "Galton showed that the heights of members of a family 'regress to the mean', and that the height of the parents does not determine the heights of their adult children. In this exercise, we will take a different approach to analyzing these data. We will use the t-test to compare the difference in means of the heights of parents and their adult children. \n",
    "\n",
    "As a first step, load the data from the .csv file provided and have a look at the first few rows of the data frame by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "GaltonFamilies = pd.read_csv('https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/GaltonFamilies.csv')\n",
    "GaltonFamilies.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has 9 columns:\n",
    " 1. A case or row number\n",
    " 2. A unique code for each family in the sample\n",
    " 3. The height of the father in inches\n",
    " 4. The height of the mother in inches\n",
    " 5. The average height of the parents\n",
    " 6. The number of children in the family\n",
    " 7. A code for the each unique child in the family\n",
    " 8. The gender of the child\n",
    " 9. The height of the adult child in inches \n",
    " \n",
    " To analyze these data, we divide them into two groups based on the gender of the adult children. The code in the cell below divides the data set, plots two histograms to compare the heights of the two samples, and performs two-sided the t-test on difference of the means of these samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = GaltonFamilies.loc[GaltonFamilies.loc[:, 'gender'] == 'male', 'childHeight']\n",
    "female = GaltonFamilies.loc[GaltonFamilies.loc[:, 'gender'] == 'female', 'childHeight']\n",
    "\n",
    "test_Galton = t_test(male, female, alpha = 0.05)\n",
    "print(test_Galton)\n",
    "plot_t(male, female, test_Galton)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine these results, noticing the following:\n",
    "1. The t-statistic is large, the p-value is small, and the confidence interval of the difference of the means does not overlap zero. We can reject the null hypothesis that the means are the same. There is a low chance that the difference in means is from random variation alone.\n",
    "2. The mean of the second sample falls outside the confidence interval, confirming that there is a low chance that the difference in means is from random variation alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 5\n",
    "Compute the t-statistics and plot the histograms for the difference in means of the fathers and their adult sons. It could be the case that children are taller than their parents. To test this idea, use the `alternative = 'larger'` argument of the `t_test` function. Using this argument defines a **one-sided t-test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "father = GaltonFamilies.loc[:, 'father']\n",
    "Galton_men = t_test(male, father, alpha = 0.05, alternative = ???) # replace the ???\n",
    "print(Galton_men)\n",
    "plot_t(???, ???, ???)   # replace the ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine your results and anwer the following questions:\n",
    "1. For the one-sided test, what is the upper bound on the confidence interval? \n",
    "2. Given the value of the t-statistic, the p-value, and the confidence interval, can you reject the null hypothesis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired T-tests\n",
    "\n",
    "There is also a version of the t-test for paired data. The heights of mothers and their adult sons is an example of paired data, since each son has a specific mother. Classically, paired tests are used to compare data from before and after a medical treatment.\n",
    "\n",
    "Sometimes the paired t-test is referred to as the dependent t-test. The null hypothesis is that there is no change between after the treatment. \n",
    "\n",
    "Execute the code in the cell below and  examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Welche's test for paired data\n",
    "def t_test_paired(a, b, alpha, alternative='two-sided'):\n",
    "    from scipy import stats\n",
    "    import scipy.stats as ss\n",
    "    import pandas as pd\n",
    "    import statsmodels.stats.weightstats as ws\n",
    "    \n",
    "    diff = a.mean() - b.mean()\n",
    "\n",
    "    res = ss.ttest_rel(a, b)\n",
    "      \n",
    "    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n",
    "    confint = means.tconfint_diff(alpha=alpha, alternative=alternative, usevar='unequal') \n",
    "    degfree = means.dof_satt()\n",
    "\n",
    "    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI', 'High95CI']\n",
    "    return pd.Series([degfree, diff, res[0], res[1], confint[0], confint[1]], index = index)   \n",
    "   \n",
    "mother = GaltonFamilies.loc[GaltonFamilies.loc[:, 'gender'] == 'male', 'mother']\n",
    "Galton_paired = t_test_paired(male, mother, alpha = 0.05)\n",
    "print(Galton_paired)\n",
    "plot_t(male, mother, Galton_paired) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Your Turn 6\n",
    "You have seen that we cannot reject the null hypothesis for a difference in means of the fathers and their sons, which was only about `0.1`. It could be that this is an issue with test power. \n",
    "\n",
    "In the code cell below use the `pwr.t2n.test` function to plot 100 values of the difference in means in the range $\\{0.0, 1.0 \\}$. The sample size arguments to this function are `n1 and n2`, and the `sig.level = 0.05`. \n",
    "\n",
    "Execute your code and examine the results. How big should the difference be to have a power or 0.9? \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = len(male)\n",
    "diffs = np.arange(???, ???, 0.01) # fill in the parameters\n",
    "powers = [smsp.tt_ind_solve_power(effect_size = x, nobs1 = nsamps, \n",
    "           alpha = ???, power = None, ratio = 1.0, alternative = 'two-sided')  # replace the ???\n",
    "          for x in diffs]\n",
    "plot_power(diffs, powers, xlabel = 'Difference of means', title = 'Power vs. difference of means')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine your plot. \n",
    "With a difference in means of 0.3 approximately, what is the power of the test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for Categorical Data\n",
    "\n",
    "So far, we have been investigating tests for data with continuous values. But, many data types are categorical when we work with count statistics. What tests can we apply to these?\n",
    "\n",
    "### Pearson's Chi-Squared Test\n",
    "\n",
    "One of the earliest tests for count data was developed by Karl Pearson around 1900. Despite its age, this test is still used today. The Pearson Chi-squared test (also written as $\\chi$-squared) has the following properties:\n",
    "\n",
    "- It is an unpaired test for counts in different categories:\n",
    "  - These categories must be mutually exclusive. For example, does the patient have cancer? (yes/no)\n",
    "  - Test if the two categories differ in WBC (White Blood Cells) count\n",
    "  - Rolling a die (1,2,3,4,5,6)\n",
    "  - Test if the six categories occur the same (fair die)\n",
    "  - Test if a tweet contains a specific word (yes/no)\n",
    "  - Test if the two categories differ in tweet length or word count\n",
    "- Tests if different categories differ in some specific value\n",
    "- The chi-squared statistic depends on the ‘degrees of freedom’ of the test.\n",
    "  - This is equal to n-1 where n equals the number of different categories.\n",
    "- The test looks at the sum of the outcome differences from expectations.\n",
    "- Chi-squared is also used for a ‘goodness of fit’ test to test if sample is representative of population.\n",
    "  - Test if your sample has expected makeup of categories.\n",
    "  - For example, if our population is 50-50 men-women, then we test if our sample is different from those expected probabilities.\n",
    "\n",
    "The density of the $\\chi$-squared distribution depends on the degrees of freedom.\n",
    "\n",
    "![](https://library.startlearninglabs.uw.edu/DATASCI410/img/Chi-square.png)\n",
    "\n",
    "As with any probability density function, confidence intervals and p-values can be computed. Notice that the $\\chi$-squared distribution becomes flatter and with greater dispersion as the degrees of freedom increase. In practice, this means that you will need large samples to get a meaningful result if you have too many choices in your test. \n",
    "\n",
    "### Karl Pearson\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Karl_Pearson_1912.jpg\" title=\"Karl Pearson in 1912: A scary looking stats professor.\" alt=\"Pearson 1912\" style=\"float: left; height: 200px; margin-right: 10px;\" />\n",
    "Karl Pearson was the dean of late 19th and early 20th century mathematical statisticians. Pearson was a student of Francis Gaulton, the inventor of the regression method. William Sealy Gossett was one of Pearsons's students. This relationship is said to be the inspiration for Gossett's pseudonym 'Student'. \n",
    "\n",
    "\n",
    "In many ways Pearson's influence on the mathematical foundations of statistics is still with us more than a century latter. Most unfortunately, Pearson was also a eugenicist and a racist. His misuse of statistics in this area tarnished his legacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Chi-squared Table\n",
    "\n",
    "The traditional way to apply a chi-squared test is to first create a chi-squared table. In this example we are looking the results of an A-B test with three possible outcomes. For example, this type of test might be applied to determine if a new web site drives more customer purchases. \n",
    "\n",
    "The code in the cell below builds a simple chi-squared table. The columns in the data frame are:\n",
    "\n",
    "- The actual occurrence of events\n",
    "- The expected probability of these events; this is the distribution of the null hypothesis.\n",
    "- The expected occurrence of events given the expected probabilities\n",
    "- The difference between the occurrence and the expected number of events\n",
    "- The square of the difference\n",
    "- The squared difference normalized by the expected number of occurrences; the sum of these figures is the chi-squared statistic. \n",
    "\n",
    "Execute the code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "ab_data = pd.DataFrame({'Action':['Leave Page', 'Continue Purchase', 'Add More to Purchase'],\n",
    "                       'occurrence':[55,43,22],\n",
    "                       'expected_per':[0.6,0.3,0.1]})\n",
    "sum_occurrence = ab_data.loc[:, 'occurrence'].sum()\n",
    "ab_data.loc[:, 'expected_occurance'] = sum_occurrence * ab_data.loc[:, 'expected_per']\n",
    "ab_data.loc[:, 'diff'] = ab_data.loc[:, 'occurrence'] - ab_data.loc[:, 'expected_occurance'] \n",
    "ab_data.loc[:, 'sqr_diff'] = ab_data.loc[:, 'diff'].apply(lambda x: x**2)\n",
    "ab_data.loc[:, 'diff_expect'] = ab_data.loc[:, 'sqr_diff'].div(ab_data.loc[:, 'expected_occurance'], axis='index')\n",
    "ab_data = ab_data.append({'Action': 'Totals',\n",
    "                      'occurrence': sum_occurrence,\n",
    "                       'expected_per':  [np.nan],\n",
    "                      'expected_occurance': [np.nan],\n",
    "                      'diff': [np.nan],\n",
    "                      'sqr_diff': [np.nan],\n",
    "                      'diff_expect': ab_data.loc[:, 'diff_expect'].sum()}, \n",
    "                        ignore_index =  True)\n",
    "ab_data = ab_data[['Action', 'occurrence', 'expected_per', 'expected_occurance', 'diff', 'sqr_diff', 'diff_expect']]\n",
    "ab_data                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chi-squared table is interpreted as follows:\n",
    "\n",
    "- The $\\chi$-squared test statistic is 13.708, which is computed as the sum of the squared differences normalized by the expected occurrences.\n",
    "- The $\\chi$-squared distribution has (3 outcomes - 1) = 2 degrees of freedom.\n",
    "- Degree of freedom is the number of outcome options (3) minus 1.\n",
    "\n",
    "Run the  code in the cell below to compute the p-value of the chi-squared test with the `scipy.stata.chi2.cdf` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "1 - ss.chi2.cdf(13.7, df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - ss.chi2.cdf(0.1, df = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This p-value is rather small. Evidently, there is a small chance that the differences between the occurrences and expected occurrences are from random variation alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the foregoing example we computed the chi-squared statistic and p-value directly. In general, this is a somewhat cumbersome approach. Instead, we can use the `chisquare` function from the `scipy.stats` package as shown in the code below. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq, pvalue = ss.chisquare(ab_data.loc[:, 'occurrence'][:3], \n",
    "             ab_data.loc[:, 'expected_occurance'][:3])\n",
    "print('Chi Squared statistic = ' + str(chisq))\n",
    "print('P=value = ' + str(pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the expected and observed number of occurrences is significant. The large chi-squared value on 2 degrees of freedom and the small p-value indicate we can reject the null hypothesis that the observed occurrences follow the same distribution as the expected occurrences. There is a low chance that this difference occurs from random variation alone. \n",
    "\n",
    "Finally, we should check the power of our test. Execute the code in the cell below and examine the results. In this case, we will use the `power_divergence` function from the `scipy.stats` package. Execute this code to determine if this test has reasonable power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.power as ssp\n",
    "diffs = np.arange(start = 0.1, stop = 1.0, step = 0.01) \n",
    "powers = ssp.GofChisquarePower().solve_power(effect_size = diffs, nobs=120, n_bins = 3, alpha=0.05)\n",
    "plot_power(diffs, powers, xlabel = 'Difference', title = 'Power vs. difference') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, this test is quite powerful, and we could have detected much smaller differences between the observed and expected counts. \n",
    "\n",
    "## Your Turn 7\n",
    "As you have observed, the test is quite powerful with 120 observations. But, what if we do not have a sample this large? \n",
    "Compute and plot the power of the chi-squared test for an **effect size of 1.0** at sample sizes in the range $\\{ 10, 100\\}$. The `nobs` argument to the `GofChisquarePower` is the sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.arange(???,???, 0.9) # fill in parameters\n",
    "powers = ssp.GofChisquarePower().solve_power(effect_size = ???, nobs = ???, n_bins = 3, alpha=0.05) # replace ???\n",
    "plot_power(samples, powers, xlabel = 'Sample size', title = 'Power vs. sample size') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine your results. \n",
    "At a sample size of 30 is the power of the test approximately 1.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher's Exact Test\n",
    "\n",
    "The assumptions behind the chi-squared statistic break down when the sample size is small (e.g., number of occurrences $\\le 10$). In this case you can use Fisher's exact test. In practice, Fisher's exact test is rarely used, but it is interesting to think about it, anyway. \n",
    "\n",
    "According to the story, in 1911 Ronald Fisher worked in the same institution with a talented aquatic botanist, Dr. Muriel Bristol, who was quite particular about how her tea was served. Dr. Bristol told Fisher that she could tell the difference between cups of tea where the milk had been poured into the cup before or after the tea was poured. \n",
    "\n",
    "Fisher, was a bit skeptical. He challenged Dr. Bristol to a test. In the test Fisher prepared eight cups of tea. Four of the cups of where prepared in Dr. Bristol's preferred manner and the other four the other way. The tea was prepared out of sight of Dr. Bristol. However, she knew that there were four cups prepared each way. The order of preparation of each cup of tea was randomized. Fisher served the cups of tea to Dr. Bristol and asked her how the tea had been poured. In every case, she was correct!\n",
    "\n",
    "Fisher devised a **permutation test** to determine the likelihood that Dr Bristol could have simply guessed the correct outcome. He devised the following permutation table for the chances of success:\n",
    "\n",
    "![](https://library.startlearninglabs.uw.edu/DATASCI410/img/tea.png)\n",
    "\n",
    "The possible permutations of possible ways that Dr Bristol could have correctly (success) or incorrectly identified the way each cup of tea was prepared:\n",
    "\n",
    "$$\\frac{8!}{4!(8-4)!} = 70$$\n",
    "\n",
    "So, the chance that the Dr Bristol could purely guess (by random chance) the outcome is only 1 in 70 or about 1.4%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an example of Fisher's exact test. Take a simple 2X2 matrix of counts. The counts are number of success and failures for two samples (two sets of Bernoulli trials) on the number of sharks observed in two oceans over some period of time. Run the code in the cell below to apply the Fisher exact test to these counts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_test = pd.DataFrame(np.array([[8,2],[1,5]]),\n",
    "                       index = ['wales', 'sharks'],\n",
    "                       columns = ['Atlantic', 'Indian'])\n",
    "print(mat_test)\n",
    "oddsratio, pvalue = ss.fisher_exact(mat_test)\n",
    "print('\\nOddsratio = ' + str(oddsratio))\n",
    "print('P-value = ' + str(pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can reject the null hypothesis. Evidently the difference of counts of whales and sharks in the two oceans from this experiment is unlikely to arise by chance alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing Summary\n",
    "\n",
    "- If data is normal:\n",
    "  - If you know population mean and variance, use standard normal ‘z-test’.\n",
    "  - If you just know population mean, use t-test.\n",
    "  - If you don't know the mean and variance, use Welch’s t-test.\n",
    "  - Test for paired and unpaired data.\n",
    "- For categorical comparison tests,\n",
    "  - If the sample/subgroup size is large enough, use chi-squared test\n",
    "  - If the sample/subgroup size is small, use Fisher’s exact test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Distributions\n",
    "\n",
    "Test statistics, and other statistics, often make distribution assumptions. This raises the question, how can we test a distribution assumption. To answer this question we will now look at both graphical and  formal tests. Since in most cases, distribution assumptions are approximate, simple graphical methods are often sufficient.   \n",
    "\n",
    "### An example with Q-Q plot\n",
    "\n",
    "The quantile-quantile (Q-Q) plot provides a handy visual means to inspect the similarity of distributions of a data set. The general idea is to plot the quantiles of the sample on the vertical axis and the quantiles of the theoretical distribution on the horizontal axis. If the points of the plot fall on an approximately straight line, you can conclude that the sample distribution is close to the theoretical. \n",
    "\n",
    "The normal Q-Q plot plots the quantiles of a standard normal distribution on the horizontal axis and the quantiles of the data sample on the vertical axis. If the sample is normal the data points will fall in a straight line. \n",
    "\n",
    "Execute the code in the cell below to compute two samples from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as nr\n",
    "nr.seed(1357)\n",
    "norm1 = nr.normal(size = 100).tolist()\n",
    "norm2 = nr.normal(size = 100).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code in the cell below to create Q-Q plots of the two samples, and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def qq_plot(vec1, vec2):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy.stats as ss\n",
    "    plt.figure(figsize=(6, 6)).gca() # define axis\n",
    "    ax1 = plt.subplot(221) ## setup the left subplot\n",
    "    ss.probplot(vec1, plot = ax1) ## Plot the left subplot\n",
    "    ax1 = plt.subplot(222) ## Set up the right subplot\n",
    "    ss.probplot(vec2, plot = ax1) ## Plot the right subplot\n",
    "qq_plot(norm1, norm2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of points on both Q-Q plots is close to straight, showing the not surprising result given that both samples are realizations of normal distributions.\n",
    "\n",
    "You can also test if two samples have similar distributions using the same plot. Run the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x = sorted(norm1), y = sorted(norm2))\n",
    "plt.xlabel('Quantiles of first distribution')\n",
    "plt.ylabel('Quantiles of second distribution')\n",
    "plt.title('Q-Q plot of two distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not surprisingly, the points line close to a straight line. The noticeable deviations are a result of the small sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kolmogorov-Smirnov Test for Distributions\n",
    "\n",
    "The Kolmogorov-Smirnov test is a general test of the distribution of a sample. The K-S statistic is just the maximum vertical distance between CDF (cumulative distribution function) of the sample and the CDF of the theoretical distribution. Since it is based on a simple deviation, the K-S test can test departure from any hypothetical distribution, not just normal. As you should expect, the trade-off for this generality is that the K-S test does not have particularly high power. There are numerous distribution-specific tests with greater power, but will will not discuss those in this lesson. \n",
    "\n",
    "As a first step in investigating the K-S test, plot the cumulative distribution functions of the two samples by executing the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_cums(dist1, dist2):\n",
    "    ## sort the first data sample:\n",
    "    data_sorted = np.sort(dist1)\n",
    "    # calculate the proportional values of samples\n",
    "    p = 1. * np.arange(len(dist1)) / (len(dist1) - 1)\n",
    "\n",
    "    # Now plot as line and scatter plot. \n",
    "    plt.plot(data_sorted, p)\n",
    "    plt.scatter(data_sorted, p, color = 'red')\n",
    "    \n",
    "    # sort the seond data sample:\n",
    "    data_sorted = np.sort(dist2)\n",
    "    \n",
    "    # And the second plot\n",
    "    plt.plot(data_sorted, p, color = 'yellow')\n",
    "    plt.scatter(data_sorted, p, color = 'green')\n",
    "\n",
    "plot_cums(norm1, norm2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, as you might expect, the two cumulative density function are quite similar. Even with data sampled from the same theoretical distribution, you can see there is deviation between the cumulative distribution functions. The question is, are these deviations significant? \n",
    "\n",
    "The **K-S statistic is the maximum vertical difference** between the two cumulative density functions. Based on this distance and the number of samples, the p-value for the K-S test is computed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the affect of normalization or standardization of the samples for the K-S test.  It is important that the **samples must be standardized** before applying the K-S test. Execute the code in the cell below to compute and plot the cumulative density function of normal distributions with different location (mean) and scale (standard deviation) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(155)\n",
    "norm3 = nr.normal(loc = 1, scale = 2.0, size = 100).tolist()\n",
    "plot_cums(norm1, norm3)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plot. You can see that there are significant deviations between the two cumulative distribution functions. This result indicates that the two samples are drawn from different distributions. \n",
    "\n",
    "Now, execute the code in the cell below to standardize (zero mean, unit standard deviation) the two samples and plot the cumulative distribution function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "norm1_s = scale(norm1)\n",
    "norm3_s = scale(norm3)\n",
    "plot_cums(norm1_s, norm3_s)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cumulative density plots of the standardized data are quite similar. This indicates the two samples are drawn from the same distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will compute the **K-S statistic, maximum vertical distance between the two cumulative density functions**, and the **p-value or probability that this difference arises from chance alone**. The code below uses the `kstest` function from the `scipy.stats` package. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_test(dat, dist = 'norm'):\n",
    "    from scipy.stats import kstest \n",
    "    ks_statisic, pvalue = kstest(dat, dist)\n",
    "    print('KS-statistic = ' + str(ks_statisic))\n",
    "    print('P-value = ' + str(pvalue))\n",
    "ks_test(norm1_s)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-S statistic is quite small and the p-value is large. The chance is high that we cannot reject the null hypothesis since the probability that the difference arises by chance alone is high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how the KS-test works for two quite different distributions, the normal and uniform. The code is the cell below computes the KS-statistics to test the normality of a uniform distribution. Execute this code and examine the results. Do these results appear as you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif = nr.uniform(size = 100).tolist()\n",
    "ks_test(unif) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the K-S statistic is large and the p-value quite small. We can reject the null hypothesis that these distributions are the same, and the chance of this difference of arising from random variation alone is quite small.\n",
    "\n",
    "Now, we and test if the samples drawn from the uniform distribution are consistent with the theoretical uniform distribution. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_test(unif, 'uniform') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we cannot reject the null hypothesis that these distributions are different. The K-S statistic is small and the p-value is large.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-S test is rather general, as it can be applied to test any distribution. However, this means that the **power** of this test is limited. The power of a test is the probability of rejecting a null hypothesis when the alternative is true. \n",
    "\n",
    "As is always the case with classical statistics, a more powerful test can be created by adopting more restrictive assumptions. For example, the Shapiro-Wilk test has greater power, but is specifically for normal distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Multiple Groups and ANOVA\n",
    "\n",
    "So far, we have only looked at tests for comparing two samples. What if we have multiple groups and we wanted to compare their means? Why can’t we just do multiple two-sample t-tests for all pairs?\n",
    "- Results in increased probability of accepting a false hypothesis; e.g., if we had 7 groups, there would be (7 Choose 2)=21 pairs to test.  If our alpha cutoff is 5%, then we are likely to accept about 1 false hypothesis (approximately 21*0.05).\n",
    "\n",
    "There is another alternative:\n",
    "\n",
    "- Null Hypothesis: All groups are samples from the same population.\n",
    "- Alternative Hypothesis: At least one group has a statistically different mean.\n",
    "\n",
    "This type of analysis is called “ANalysis Of VAriance”, or ANOVA. ANOVA is one of a large family of models used for **experimental design**.\n",
    "\n",
    "### History of ANOVA\n",
    "\n",
    "ANOVA is not a new idea. \n",
    "\n",
    "- Laplace pioneered multiple comparison methods in 1827.\n",
    "- Ronald A Fisher published seminal work inn 1922, 1925, and 1935. The F (Fisher) statistic is named in his honor.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Ronald_Fisher.jpg\" title=\"Ronald A Fisher, another scary-looking stats professor!\" alt=\"Fisher image\" style=\"float: left; height: 200px; margin-right: 10px;\" />\n",
    "Fisher pioneered the use of linear models for testing multiple groups (ANOVA) and the design of experiments to ensure meaningful analysis. Fisher had an overwhelming influence on the theory of classical (frequentist) statistics. He was vehemently opposed to Bayesian methods, and ostracized any practitioners. In fact, Fisher's long shadow explains why we are only beginning to teach Bayesian methods in the 21st century. Unfortunately, as with Pearson, Fisher was also a eugenicist and a racist.\n",
    "\n",
    "Fisher's two books are still influential and in print. \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Fisher1.jpg\" alt=\"Fisher 1935 book\" title=\"Fisher's 1935 book\" style=\"display: inline; height: 200px;margin: 5px;\" /><img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Fisher2.jpg\" alt=\"Fisher 1925 book\" title=\"Fisher's 1925 book\" style=\"display: inline; height: 200px; margin: 5px;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ANOVA Theory \n",
    "\n",
    "Let's have a look at how we would perform the comparisons between the multiple groups of data. First, make data independence and normality assumptions about the groups. Then define:\n",
    "\n",
    "$$ I = number\\ of\\ treatments\\\\\n",
    "n = number\\ of\\ data\\ or\\ samples\\\\\n",
    "SS = sum\\ of\\ squares$$\n",
    "\n",
    "We can calculate the following quantities:\n",
    "\n",
    "$$SST = SS\\ Treatment\\\\\n",
    "SSE = SS\\ Error\\ within\\ groups\\\\\n",
    "SS- SSTotal = SST + SSE$$\n",
    "\n",
    "Further, \n",
    "\n",
    "$$ DFT = degrees\\ of\\ freedom\\ Treatment\\\\\n",
    "DFE = degrees\\ of\\ freedom\\ Error\\ within\\ groups\\\\\n",
    "DFTotal = DFT + DFE = (I-1) + (n-I) = n -1$$\n",
    "\n",
    "And,\n",
    "\n",
    "$$MST = mean\\ square\\ error\\ Treatment\\\\\n",
    "MSE = mean\\ square\\ error\\ within\\ groups$$\n",
    "\n",
    "Finally we can compute the F statistic with $I-1$ degrees of freedom:\n",
    "\n",
    "$$F = \\frac{Variance\\ between\\ treatments}{Variance\\ within\\ treatments} = \\frac{MST}{MSE} =  \\frac{\\frac{SST}{DFT}}{\\frac{SSE}{DFE}}$$\n",
    "\n",
    "The F statistic on the degrees of freedom determines the significance or p-values of the test. We can lay these results out in an ANOVA table as follows:\n",
    "\n",
    "|Type|Sum of Squares|df|Mean Square E|F|Significance|\n",
    "|---|---|---|---|---|---|\n",
    "|Between Groups|SST|DFT|SST/DFT|F Statistic| p-value|\n",
    "|Within Groups|SSE|DFE|SSE/DFE|||\n",
    "|Groups Total|SSTotal|DFTotal||||\n",
    "\n",
    "\n",
    "### ANOVA Example\n",
    "\n",
    "Let's start with an example with 4 groups. In Fisher's experimental design terminology we say we have data from 4 **treatments**. Run the code below and examine the difference in the box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(335566)\n",
    "df1 = nr.normal(size = 50).tolist()\n",
    "df2 = nr.normal(size = 50).tolist()\n",
    "df3 = nr.normal(loc = 0.5, size = 60).tolist()\n",
    "df4 = nr.normal(size = 40).tolist()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot([df1, df2, df3, df4])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Variable')\n",
    "plt.title('Box plot of variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows variation between the distributions of the four variables. The question is, are these differences significant? \n",
    "\n",
    "The code in the cell below applies the R `f_oneway` function, from the `scipy.stats` package, to the data. The `f_oneway` function computes an F-Statistic and a p-value. \n",
    "\n",
    "The code in the cell below models the ANOVA of the values by groups and prints the results of the model. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic, p_value = ss.f_oneway(df1, df2, df3, df4)\n",
    "print('F statistic = ' + str(f_statistic))\n",
    "print('P-value = ' + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F statistic is fairly large and the p-value is small. We can reject the null hypothesis that the 4 variables have the same mean, as the probability of the differences arising from random chance is quite low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the question of the power of this ANOVA test. In other words, what is the probability that we will detect a difference in means? \n",
    "\n",
    "The code in the cell below uses the `FTestAnovaPower.solve_power` function from the `statsmodels.stats.power` package. Power is computed for mean differences in the range $\\{ 0.1, 1.0 \\}$ and plotted against the mean difference. To be conservative, we are using the smallest number of samples for the variables as the number of observations, `nobs`. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.power as ssp\n",
    "diffs = np.arange(start = 0.1, stop = 1.0, step = 0.01) \n",
    "powers = ssp.FTestAnovaPower().solve_power(effect_size = diffs, nobs=40, alpha=0.05)\n",
    "plot_power(diffs, powers, xlabel = 'Difference', title = 'Power vs. difference') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even with 40 observations, the probability of detecting a fairly small difference in means between the groups is quite high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 8\n",
    "In a hypothetical example, after one month on the job, a new manager at an auto dealership observes changes in the average daily total sales by day of the week. She wants to know if these differences are significant or just from random variation.\n",
    "\n",
    "To solve the problem you will do the following:\n",
    "1. Execute the code in the cell provided below to compute some simulated data values by day of the week an display a box plot. The parameters for the normal distributions for each day of the week are based on the average sales for each day and the standard deviation of sales over the month. \n",
    "2. In the next cell compute and display the F statistic and p-value for this sample. \n",
    "3. Compute the power of this test with the  following parameters:\n",
    "  - Range of differences from 1.0 to 10 in steps of 0.1. \n",
    "  - To display the plot of power vs. dollars, you must scale these differences by 10,000, the scale of the normal distribution. Do this after you have computed the power values. This process is necessary since the manager will want to see the results in units she understands, dollars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr.seed(998877)                                       # simulated data\n",
    "Mon = nr.normal(250000, scale = 10000, size = 4)\n",
    "Tue = nr.normal(255000, scale = 10000, size = 4)\n",
    "Wed = nr.normal(245000, scale = 10000, size = 4)\n",
    "Thu = nr.normal(260000, scale = 10000, size = 4)\n",
    "Fri = nr.normal(265000, scale = 10000, size = 4)\n",
    "Sat = nr.normal(275000, scale = 10000, size = 4)\n",
    "Sun = nr.normal(270000, scale = 10000, size = 4)\n",
    "\n",
    "plt.boxplot([Mon, Tue, Wed, Thu, Fri, Sat, Sun])\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Day of week')\n",
    "plt.title('Box plot by day of week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_statistic, p_value = ss.f_oneway(Mon, Tue, Wed, Thu, Fri, Sat, Sun) # fill in parameters\n",
    "print('F statistic = ' + str(f_statistic))\n",
    "print('P-value = ' + str(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "diffs = np.arange(???) # fill in parameters\n",
    "powers = ssp.FTestAnovaPower().solve_power(effect_size = diffs, nobs=4, alpha=0.05)\n",
    "plot_power(???, powers, xlabel = 'Difference', title = 'Power vs. difference') # replace the ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the results of you analysis and answer the following questions:\n",
    "1. Is the difference between the sales on the different days statistically significant at the 95% level? \n",
    "2. For a price difference of $60,000, what is the approximate power of this test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turkey's ANOVA: Telling Groups Apart\n",
    "\n",
    "From the above ANOVA results we know that there is some difference in the means of these variables. However, the ANOVA does not tell us which variable is significantly different. From the box plot, we could guess it that group 3 is different, but we really don't know.\n",
    "\n",
    "John Tukey proposed a test, which he dubbed the HSD, or Honest Significant Differences, test. The test exhaustively computes the following for each pair of groups:\n",
    "- Difference of the means\n",
    "- Confidence interval of the difference in the means\n",
    "- A p-value from the distribution of the differences\n",
    "\n",
    "These results are laid out in a table or can be plotted graphically. Only differences in means with a confidence interval not overlapping zero are considered significant.\n",
    "\n",
    "The cells below contain the code to compute the Tukey HSD for the example. The code uses the `pairwise_tukeyhsd` function from the `statsmodels.stats.multicomp` package. Run this code and examine the results to determine which differences are significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "df = pd.DataFrame({'vals': df1 + df2 + df3 + df4,\n",
    "                   'group_num': ['1'] * 50 + ['2'] * 50 + ['3'] * 60 + ['4'] * 40})\n",
    "Tukey_HSD = pairwise_tukeyhsd(df.vals, df.group_num)\n",
    "print(Tukey_HSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the table above. If the difference in means between the variables is significant, the confidence interval will not include 0. Which, pairs have a significant difference at the 95% confidence level? You can see the results of this test in the left most column of the table. \n",
    "\n",
    "The `plot_simultaneous` method for a `pairwise_tukeyhsd` object allows you to create a plot of the test results. Plot these figures and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tukey_HSD.plot_simultaneous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the plot above. There is a line with a dot shown for each variable. The dot is the mean and the line shows the range of the confidence interval for  that mean. If the difference in means is significant at the confidence level, the confidence intervals will not overlap. Which pairs in the above plot have a significant difference at the 95% confidence level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 9\n",
    "It would be useful for the manager of the auto dealership understand which days of the week have significantly different average sales at the 95% confidence level. To solve this problem do the following:\n",
    "1. Compute and print the results of the Tukey HSD test using the `pairwise_tukeyhsd` function. To do so, you will need to first create a list of sales data by concatenating the sales by day of the week using the `numpy.concatenate` function, and then apply the `flatten` method. Since these values are numpy arrays you cannot use the `+` operator for concatenation. You will also need a list of days of the week, which can be created using the normal multiplication, `*`, and concatenation, `+` operators. \n",
    "2. Use the `plot_simultaneous` method on your model object to display the confidence intervals of the means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "sales = ???\n",
    "groups = ['Mon'] * 4 + ['Tue'] * 4 + ['Wed'] * 4 + ['Thu'] * 4 + ['Fri'] * 4 + ['Sat'] * 4 + ['Sun'] * 4\n",
    "Tukey_auto = pairwise_tukeyhsd(???) # fill in the parameters\n",
    "print(Tukey_auto)\n",
    "Tukey_auto.plot_simultaneous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which four pairs days of the week are statistically different at the 95% confidence level? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[i,j] for i,j in zip(sales, groups)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Tests and Big Data\n",
    "\n",
    "With the increasing scale of data sets in the 21st Century, problems with the correct application of classical tests have been amplified. The primary issue is with false positives, or **Type I errors**. This problem is known as the problem of **multiple comparisons** or **base rate fallacy**. \n",
    "\n",
    "To understand this problem lets look at a simple example. If we have 7 groups, there are 21 possible interactions between the groups. If we use a t-test with a significance level of 0.05, we expect there to be one false positive, even if the null hypothesis is true in all cases. \n",
    "\n",
    "These problems grow as the number of groups grow. As an example, consider classifying several millions objects from images. \n",
    "\n",
    "When there are relatively few groups with a great many cases, there is no problem. The test will have ample power.\n",
    "\n",
    "With large numbers of groups, there is a high probability of getting a false positive, or type I error, with ANOVA. Without any way to know which groups are different, it is nearly impossible to determine which group may or may not be different from the rest of the population. This is a manifestation of the base rate fallacy. \n",
    "\n",
    "### Bonferroni Correction  \n",
    "\n",
    "Several adjustments to the multiple comparisons problem have been proposed. In 1979 Holm published a method know as the **Bonferroni correction**. The adjustment is simple:\n",
    "\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "\n",
    "The problem with the Bonferroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n",
    "\n",
    "### Confidence and Credible Intervals\n",
    "\n",
    "Another approach to the base rate fallacy problem is to use **confidence intervals**. For example, one can compute the confidence intervals of the distributions of differences of means between groups. We have just seen an example with Tukey's HSD method. \n",
    "\n",
    "Interpretation of confidence intervals is quite straight forward, especially when compared to p-values. If the confidence intervals for the difference in means do not overlap 0, the difference in means is significant. \n",
    "\n",
    "The confidence intervals can be adjusted analogously to the Bonferroni correction. This process does not have the problem with the reduction in power.   \n",
    "\n",
    "Yet another approach is to use the Bayesian **credible interval**. While the computation is different, the interpretation is intuitive, and analogous to interpretation of confidence intervals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have covered lot of ground in this lesson. Specifically we have discussed:\n",
    "\n",
    "- Mean comparison tests in the form of the t-test. In this case, the null hypothesis is that there is no significant difference in means and the samples are from the same population.  \n",
    "- The $\\chi$-squared test for count data. The null hypothesis is that there is no significant differences in the counts and the samples are from the same population. \n",
    "- Distribution comparison tests in the form of the K-S test. The null hypothesis is that the two distributions are the same. \n",
    "- Variance comparison test for multiple grouped in the form of ANOVA. The null hypothesis is that there are no differences in the variances of the samples and they are all from the same population. \n",
    "- The meaning of the p-value or significance level. A p-value is the probability of obtaining an effect at least as extreme as the one in your sample data, assuming the null hypothesis is true.\n",
    "- Power of tests. The power of a test is the probability of getting a positive result when the null hypothesis is not true. \n",
    "- Multiple comparison issues with applying tests to big data problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Material\n",
    "\n",
    "If you need some review of the concepts of classical statistics there are numerous good books available. Two particularly non-mathematical and readable books are shown below. These books have distinct and different styles. If you do not like one, you might like the other. Or, you may benefit from reading both. \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/StatsDoneWrong.jpg\" title=\"Reinhart, 2015\" alt=\"Reinhart's book\" style=\"float: left; height: 200px; margin-right: 10px;\"/><img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/p-value.jpg\" title=\"Vicker, 2009\" alt=\"Vicker's book\" Style=\"float: right; height: 200px; margin-left: 10px;\" />\n",
    "Alex Reinhart's book, *Statistics Done Wrong, a woefully complete guide*, No Starch Press, 2015, discusses classical statistical tests using numerous clever examples. \n",
    "\n",
    "Andrew Vicker's *What is a P-Value Anyway*, Pearson, 2009, is perhaps the most humorous statistics book ever written; admittedly a category with little competition. The treatment is a bit broader than Reinhart's book. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "342.367px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
