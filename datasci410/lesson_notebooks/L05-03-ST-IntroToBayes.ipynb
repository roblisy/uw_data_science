{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "Version: Jun-2019\n",
    "<p>Look for the <b>3 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayes Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Modern Bayesian models are in the class of computationally intensive models. Bayesian models are a rich class of models, which can provide attractive alternatives to frequentist models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief history\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/ThomasBayes.gif\" style=\"float:left; padding-right: 10px; height: 100px;\" title=\"Thomas Bayes\" />A restricted version of Bayes Theorem was proposed by Rev.Thomas Bayes (1702-1761). Bayes Theorem was published posthumously by his friend Richard Price. Bayes' interest was in probabilities of gambling games. He was also a supporter of Issac Newton's new theory of calculus with his publication, *An Introduction to the Doctrine of Fluxions, and a Defence of the Mathematicians Against the Objections of the Author of The Analyst*.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/Laplace.jpg\" style=\"float:right; padding-left: 10px; height: 120px;\" title=\"Pierre-Simon Laplace\" />\n",
    "\n",
    "\n",
    "Pierre-Simon Laplace published a version of Bayes Theorem similar to its modern form in his *Essai philosophique sur les probabilit√©s* 1814. Laplace applied Bayesian methods to problems in celestial mechanics. These problems had great practical implications in the late 18th and early 19th centuries for the safe navigation of ships. \n",
    " \n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/JeffreysProbability.jpg\" style=\"float: left; padding: 10px; height: 120px;\" title=\"Jeffery's 1939 Probability book\" />\n",
    "The geophysicist and mathematician Harold Jeffreys extensively used Bayes' methods. His 1939 book, *The Theory of Probability* was in deliberate opposition to Fisher's methods using p-values.\n",
    "\n",
    "\n",
    "\n",
    "Despite the philosophical squabbles, Bayesian methods endured and showed an increasing number of success stories. Pragmatists continued to use both approaches. A number of successes during the Second World War, with the philosophical battles raging, included:\n",
    "\n",
    "- Bayesian models were used to improve artillery accuracy in both world wars. In particular the Soviet statistician Andrey Kolmagorov used Bayes methods to greatly improve artillery accuracy. \n",
    "- Bayesian models were used by Alan Turing to break German codes.\n",
    "- Bernard Koopman, working for the British Royal Navy, improved the ability to locate U-boats using directional data from intercepted radio transmissions. \n",
    "\n",
    "\n",
    "Starting in the second half of the 20th century the convergence of greater computing power and general acceptance lead to the following notable advances in computational Bayesian methods.\n",
    "\n",
    "- Statistical sampling using Monte Carlo methods; Stanislaw Ulam, John von Neuman; 1946, 1947\n",
    "- MCMC, or Markov Chain Monte Carlo; Metropolis et al. (1953) Journal of Chemical Physics\n",
    "- Hastings (1970), Monte Carlo sampling methods using Markov chains and their application\n",
    "- Geman and Geman (1984) Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images\n",
    "- Duane, Kennedy, Pendleton, and Roweth (1987),  Hamiltonian MCMC\n",
    "- Gelfand and Smith (1990), Sampling-based approaches to calculating marginal densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian vs. Frequentist Views\n",
    "\n",
    "The battle between Fisher, Jeffreys, and their prot√©g√©s continued for most of the 20th century. This battle was bitter and often personal. The core of these argument were:\n",
    "\n",
    "- Fisher argued that the selection of a Bayesian prior distribution was purely subjective, allowing one to achieve any answer desired.\n",
    "- Jeffreys argued that all knowledge is in fact subjective, and that choosing a confidence interval was subjective in any event.\n",
    "\n",
    "With greater computational power and general acceptance, Bayes methods are now widely used in areas ranging from medical research to natural language understanding to web search. Among pragmatists, the common belief today is that some problems are better handled by frequentist methods and some with Bayesian methods.\n",
    "\n",
    "Let's summarize the differences between the Bayesian and frequentist views: \n",
    "\n",
    "- Bayesian methods use priors to quantify what we know about parameters.\n",
    "- Frequentists do not quantify anything about the parameters, using p-values and confidence intervals to express the unknowns about parameters.\n",
    "\n",
    "Recalling that both views are useful, we can contrast these methods with a chart.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/FrequentistBayes.jpg\" title=\"Frequentists versus Bayes\" style=\"height: 500px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Some Introductory Texts\n",
    "\n",
    "These two books provide a broad and readable introduction to Bayesian data analysis. Well, sort of. Both books contain extensive examples using R and specialized Bayes packages.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/StatisticalRethinking.jpg\" style=\"display: inline; height: 200px; margin: 5px;\" title=\"Statistical Rethinking\" /> <img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/DoingBaysianDataAnalysis.jpg\" style=\"display: inline; height: 200px; margin: 5px;\" title=\"Doing Bayesian Data Analysis\" />\n",
    "\n",
    "\n",
    "\n",
    "### Modeling Reference\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/BayesRegression.jpg\" style=\"height: 200px; float: left; padding-right: 10px;\" title=\"Data Analysis using Regression and Multilevel Hierarchical Models\" />\n",
    "This book contains a comprehensive treatment of applying Bayesian models. The level of treatments in intermediate. The examples are from the social sciences, but the methods can be applied more widely. The examples use R and specialized Bayes packages. \n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "### Theory \n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/BaysianDataAnalysis.jpg\" style=\"height: 200px; float: left; padding-right: 10px;\" title=\"Bayesian Data Analysis\" />\n",
    "This book contains a comprehensive overview of the modern theory of Bayesian models. The book is at an advanced level. Only theory is addressed, with only very limited R code examples.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "Let's go through a simple derivation of Bayes' theorem. Remember the rule for conditional probability:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{p(B)}\\\\\n",
    "and\\\\\n",
    "P(B|A) = \\frac{P(A \\cap B)}{p(A)}$$\n",
    "\n",
    "Eliminating $P(A \\cap B):$\n",
    "\n",
    "$$ P(B)P(A|B) = P(A)P(B|A) $$\n",
    "\n",
    "$$ Or\\\\ $$\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "\n",
    "Which is Bayes' Theorem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Probabilities of Eye and Hair Color\n",
    "\n",
    "A sample population has the following probabilities of eye and hair color combinations. Execute the code to see the chart of conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_hair = pd.DataFrame({\n",
    "    'black': [0.11, 0.03, 0.03, 0.01], \n",
    "    'brunette': [0.2, 0.14, 0.09, 0.05],\n",
    "    'red': [0.04, 0.03, 0.02, 0.02],\n",
    "    'blond': [0.01, 0.16, 0.02, 0.03],\n",
    "}, index=['brown', 'blue', 'hazel', 'green'])\n",
    "\n",
    "eye_hair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: here we're using a string index for eye color rather than a numeric zero-based index. So to access a given (eye, hair) color value, index the dataframe like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_hair.loc['hazel', 'red']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the table above are the **conditional probabilities**. Note that in the case: \n",
    "\n",
    "$$P(hair | eye) = P(eye | hair)$$ \n",
    "\n",
    "Given these conditional probabilities, it is easy to compute the marginal probabilities by summing the probabilities in the rows and columns. The **marginal probability** is the probability along one variable (one margin) of the distribution. For example, $P(Red)$ or $P(Green)$. Like all probability distributions, marginal distributions must sum to 1.0. \n",
    "\n",
    "The code in the cell below computes the marginal probabilities by both hair color and eye color. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the marginal distribution of each eye color\n",
    "eye_hair['marginal_eye'] = eye_hair.sum(axis=1)\n",
    "eye_hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_hair.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_hair.loc['marginal_hair'] = eye_hair.sum(axis=0)\n",
    "eye_hair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 1\n",
    "Use Bayes Theorem to compute the probability of each hair color given that the subject has blue eyes; \n",
    "\n",
    "$$P(Hair\\ Color|Blue\\ Eyes)$$ \n",
    "\n",
    "Hint, this is a bit tricky since $P(hair\\ color) = 1$ across all colors. \n",
    "<br><br>\n",
    "<div style=\"background-color: lightyellow; border-radius: 10px;\">\n",
    "    <p style=\"font-weight: bold;\">Bayes Theorem:</p><br>\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "<br>\n",
    "You've got the probabilities listed in the table above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of blue eyes given any hair color divided by total probability of blue eyes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bayes Theorem\n",
    "\n",
    "We need a formulation of Bayes Theorem that is convenient to use for computational problems. Specifically, we don't want to be stuck summing all of the possibilities to compute $P(B)$. \n",
    "\n",
    "Look at some fun facts about conditional probabilities. \n",
    "\n",
    "$$\n",
    "P(B \\cap A) = P(B|A)P(A)\\\\\n",
    "And \\\\\n",
    "P(B) = P(B |\\cap A) | P(B \\cap \\bar{A}) \\\\\n",
    "Then \\\\\n",
    "P(B) = P(B|A) P(A)|P(B| \\bar{A})P(\\bar{A}) \\\\\n",
    "where\\\\\n",
    "\\bar{A} = Not\\ A\n",
    "$$\n",
    "\n",
    "\n",
    "We can now rewrite Bayes Theorem:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{ùëÉ(ùêµ‚îÇùê¥)ùëÉ(ùê¥)+ùëÉ(ùêµ‚îÇ \\bar{ùê¥})ùëÉ(\\bar{ùê¥})} \\\\ $$\n",
    "\n",
    "This is a bit of a mess. But fortunately, we don't always need the denominator. We can rewrite Bayes Theorem as:\n",
    "\n",
    "$$ùëÉ(ùê¥‚îÇùêµ)=ùëò‚àôùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥)$$\n",
    "\n",
    "Ignoring the normalization constant $k$, we get:\n",
    "\n",
    "$$ùëÉ(ùê¥‚îÇùêµ) \\propto ùëÉ(ùêµ|ùê¥)ùëÉ(ùê¥)$$\n",
    "\n",
    "### Applying the Simplified relationship Bayes Theorem\n",
    "\n",
    "How to we interpret the relationships shown above? We do this as follows:\n",
    "\n",
    "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
    "Or\\\\\n",
    "P(parameters|data) \\propto P(data|parameters)P(parameters) $$\n",
    "\n",
    "\n",
    "These relationships apply to the observed data distributions, or to parameters in a model (partial slopes, intercept, error distributions, lasso constant,‚Ä¶). \n",
    "\n",
    "### Creating Bayes Models\n",
    "\n",
    "Given prior assumptions about the behavior of the parameters (the prior), produce a model which tells us the probability of observing our data, to compute new probability of our parameters. Given this, the steps for working with a \n",
    "\n",
    "- Identify data relevant to the research question: e.g., what are the measurement scales of the data?\n",
    "- Define a descriptive model for the data. For example, pick a linear model formula.\n",
    "- Specify a prior distribution of the parameters. For example, we think the error in the linear model is Normally distributed as $N(\\theta,\\sigma^2)$.\n",
    "- Use the Bayesian inference formula (above) to compute posterior parameter probabilities.\n",
    "- Update if more data is observed. This is key! The posterior of a Bayesian model naturally updates as more data is added, a form of learning.\n",
    "- Simulate data values from realizations of the posterior distribution of the parameters.\n",
    "\n",
    "\n",
    "### How do we choose a prior?\n",
    "\n",
    "The choice of the prior is a serious problem when performing Bayesian analysis. In general, a prior must be convincing to a **skeptical audience**. Some possible approaches include:\n",
    "\n",
    "- Prior observations\n",
    "- Domain knowledge\n",
    "- If poor knowledge use less informative prior\n",
    "- **Watch out:** A uniform prior is informative. For example, you must set the limits on range of values.\n",
    " \n",
    "One analytically and computationally simple choice is a **conjugate prior**. When a likelihood is multiplied by a conjugate prior the distribution of the posterior is the same as the likelihood. Most named distributions have conjugates. A few commonly used examples are shown in the table below:\n",
    "\n",
    "Likelihood | Conjugate\n",
    "---|---\n",
    "Binomial|Beta\n",
    "Bernoulli|Beta\n",
    "Poisson|Gamma\n",
    "Categorical|Dirichlet\n",
    "Normal| Normal, Inverse Gamma\n",
    "\n",
    "However, there are many practical cases where a conjugate prior is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example\n",
    "\n",
    "Let's try a first example. \n",
    "\n",
    "With a bit of theory in mind, let's pull things together with an example. Let's say we are interested in analyzing distracted drivers. We sample the behavior of 10 drivers at an intersection and determine if they exhibit distracted driving or not. The data are binomially distributed; a driver is distracted or not. In the example we will:\n",
    "\n",
    "- Select a prior for the parameter $p$, the probability of distracted driving.\n",
    "- Using data, compute the likelihood.\n",
    "- Compute the posterior and posterior distributions. \n",
    "- Try another prior distribution.\n",
    "- Add more data to our data set to updated the posterior distribution.\n",
    "\n",
    "The likelihood of the data and the posterior distribution are binomially distributed. The binomial distribution has one parameter we need to estimate, $p$, the probability. We can write this formally for $k$ successes in $N$ trials:\n",
    "\n",
    "$$ P(A) = \\binom{N}{k} \\cdot p^k(1-p)^{N-k}$$\n",
    "\n",
    "The code in the cell below creates a simple data set of distracted drivers and computes some simple summary statistics. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = ['yes','no','yes','no','no','yes','no','no','no','yes']\n",
    "distracted = [1 if x is 'yes' else 0 for x in drivers]\n",
    "distracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(distracted)  # sample size\n",
    "n_distracted = sum(distracted)  # number of distracted drivers\n",
    "n_not = N - n_distracted # number not distracted\n",
    "print('Distracted drivers = %d Attentive drivers = %d'\n",
    "    '\\nProbability of distracted driving = %.1f' \n",
    "      % (n_distracted, n_not, n_distracted / (n_distracted + n_not)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a prior distribution for our one model parameter $p$, $P(p)$. We don't know a lot about these drivers at this point, so we will start with a uniform distribution. \n",
    "\n",
    "The code in the cell below computes and plots the uniform prior distribution. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "p = np.linspace(.01, .99, num=N)\n",
    "pp = [1./N] * N\n",
    "plt.plot(p, pp, linewidth=2, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compute the likelihood. The likelihood is the probability of the data given the parameter, $P(X|p)$. We can view the observation of each driver as distracted or not as a Bernoulli trial, so we will use the binomial distribution. \n",
    "\n",
    "The code in the cell below computes and plots the binomial likelihood for the distracted driver data. This calculation is performed for each value of $p$ we are  sampling. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(p, data):\n",
    "    k = sum(data)\n",
    "    N = len(data)\n",
    "    # Compute Binomial likelihood\n",
    "    l = scipy.special.comb(N, k) * p**k * (1-p)**(N-k)\n",
    "    # Normalize the likelihood to sum to unity\n",
    "    return l/sum(l)\n",
    "\n",
    "l = likelihood(p, distracted)\n",
    "plt.plot(p, l)\n",
    "plt.title('Likelihood function')\n",
    "plt.xlabel('Parameter')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a prior and a likelihood we are in a position to compute the posterior distribution of the parameter $p$, $P(p|X)$. The code in the cell below computes and plots the posterior, given the prior and likelihood.\n",
    "\n",
    "***\n",
    "<span style=\"color: magenta;\">**Warning!**</span>  The computational methods used in this notebook are simplified for the purpose of illustration. For real-world problems, computationally efficient code must be used!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(prior, like):\n",
    "    post = prior * like # compute the product of the probabilities\n",
    "    return post / sum(post) # normalize the distribution to sum to unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_post(prior, like, post, x):\n",
    "    maxy = max(max(prior), max(like), max(post))\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(x, like, label='likelihood', linewidth=12, color='black', alpha=.2)\n",
    "    plt.plot(x, prior, label='prior')\n",
    "    plt.plot(x, post, label='posterior', color='green')\n",
    "    plt.ylim(0, maxy)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.title('Density of prior, likelihood and posterior')\n",
    "    plt.xlabel('Parameter value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    \n",
    "post = posterior(pp, l)\n",
    "plot_post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum of the prior density = %.3f' % max(pp))\n",
    "print('Maximum likelihood = %.3f' % max(l))\n",
    "print('MAP = %.3f' % max(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with uniform prior distribution, the posterior is just the likelihood. This is an important observation. The key point is that the frequentist probabilities are identical to the Bayesian posterior distribution given a uniform prior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Prior\n",
    "\n",
    "Let's try another prior distribution. We will chose the **conjugate prior** of the Binomial distribution which is the beta distribution. Formally, we can write the beta distribution:\n",
    "\n",
    "$$Beta(p |a, b) = \\kappa x^{a-1}(1 - x)^{b-1} \\\\\n",
    "where,\\ \\kappa = normalization\\ constant$$\n",
    "\n",
    "The beta distribution is defined on the interval $0 \\le Beta(p|a,b) \\le 1$. The beta distribution has two parameters, $a$ and $b$, which determine the shape. To get a feel for the beta distribution, execute the code in the cell below which computes 100 examples on a 10x10 grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "alpha = [.5, 1, 2, 3, 4]\n",
    "beta = alpha[:]\n",
    "x = np.linspace(.001, .999, num=100)\n",
    "\n",
    "for i, (a, b) in enumerate(itertools.product(alpha, beta)):\n",
    "    plt.subplot(len(alpha), len(beta), i+1)\n",
    "    plt.plot(x, scipy.stats.beta.pdf(x, a, b))\n",
    "    plt.title('(a, b) = ({}, {})'.format(a,b))\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the plots above, that the beta distribution can take on quite a range of shapes, depending on the parameters. Generally if $a \\gt b$ the distribution skews to the rights, if $a \\lt b$ to the left, and symmetric if $ a = b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still do not know a lot about the behavior of drivers, so we will pick a rather vague or broad beta distribution as our prior. The code in the cell below uses a symmetric prior with $a = 2$ and $b = 2$. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_prior(x, a, b):\n",
    "    l = scipy.stats.beta.pdf(p, a, b)  # compute likelihood\n",
    "    return l / l.sum()  # normalize and return\n",
    "\n",
    "pp = beta_prior(p, 2, 2)\n",
    "post = posterior(pp, l)\n",
    "plot_post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the mode of the posterior is close to the mode of the likelihood, but has shifted toward the mode of the prior. We call this tendency of Bayesian posteriors to be shifted toward the prior the **shrinkage property**. The tendency of the maximum likelihood point of the posterior is said to shrink toward the maximum likelihood point of the prior. \n",
    "\n",
    "We can now see that the posterior probability of distracted driving has a rather wide spread. How can we get a more definitive understanding of the probability of distracted driving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data to the Bayesian model\n",
    "\n",
    "Let's say that we observe some more drivers and gather some more data on distracted driving. Additional data will narrow the spread of the posterior distribution. As you might expect, adding more observations to the model, moves the posterior closer to the likelihood. \n",
    "\n",
    "In fact, as data is added to a Bayesian model, the posterior moves toward the likelihood. This property has two important implications:\n",
    "\n",
    "- The prior matters less as more data is added to a Bayesian model.\n",
    "- Adding data reduces shrinkage.\n",
    "- The inferences from Bayesian and frequentist models tend to converge as data set sizes grow and the posterior approaches the likelihood.\n",
    "\n",
    "**But, be careful!** With large scale problems with large numbers of parameters you may need enormous data sets to see the convergence in behavior. \n",
    "\n",
    "The code in the cell below adds another 10 observations to our data set. Execute this code and examine the results. How do the likelihood and posterior distributions compare with the case with only 10 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_drivers = ['no','yes','no','no','no',\n",
    "          'yes','no','yes','no','no']  # Some new data\n",
    "new_distracted = [1 if x is 'yes' else 0 for x in new_drivers]\n",
    "\n",
    "l = likelihood(p, distracted + new_distracted)\n",
    "post = posterior(pp, l)\n",
    "plot_post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credible Intervals\n",
    "\n",
    "A **credible interval** is an interval on the Bayesian posterior distribution. The credible interval is sometime called the highest density interval (HDI), or highest posterior density interval (HPI). As an example, the 90% credible interval encompasses the 90% of the posterior distribution with the highest probability density.  \n",
    "\n",
    "The credible interval is the Bayesian analog of the frequentist confidence interval. However, these two measures are conceptually different. The confidence interval is chosen on the distribution of a test statistic, whereas the credible interval is computed on the posterior distribution of the parameter. For symmetric distributions the credible interval can be numerically the same as the confidence interval. However, in the general case, these two quantities can be quite different.  \n",
    "\n",
    "The code in the cell below, plots the posterior distribution of the parameter of the binomial distribution parameter  pp . The 95% credible interval, or HDI, is also computed and displayed. Execute this code and examine the result. \n",
    "\n",
    "***\n",
    "<span style=\"color: magenta;\">**Warning!**</span> This code assumes a symmetric prior distribution, so will not work in the general case. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "lower_q, upper_q = [.025, .975]\n",
    "\n",
    "def plot_ci(p, post, num_samples, lower_q, upper_q):\n",
    "    ## This function computes a credible interval using an assumption\n",
    "    ## of symetry in the bulk of the distribution to keep the \n",
    "    ## calculation simple. \n",
    "    ## Compute a large sample by resampling with replacement\n",
    "    samples = np.random.choice(p, size=num_samples, replace=True, p=post)\n",
    "    ci = scipy.percentile(samples, [lower_q*100, upper_q*100]) # compute the quantiles\n",
    "    \n",
    "    interval = upper_q - lower_q\n",
    "    plt.title('Posterior density with %.3f credible interval' % interval)\n",
    "    plt.plot(p, post, color='blue')\n",
    "    plt.xlabel('Parameter value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.axvline(x=ci[0], color='red')\n",
    "    plt.axvline(x=ci[1], color='red')\n",
    "    print('The %.3f credible interval is %.3f to %.3f' \n",
    "          % (interval, lower_q, upper_q))\n",
    "    \n",
    "plot_ci(p, post, num_samples, lower_q, upper_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating from the  posterior distribution: forecasting\n",
    "\n",
    "So far, we have computed the posterior distribution of the probability parameter $p$. But what about the distribution of distracted drivers? We can compute this distribution by simulating from the posterior distribution of $p$. \n",
    "\n",
    "The code in the cell below simulates and plots the distribution of distracted drivers. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cars = 10\n",
    "num_samples = 10000\n",
    "\n",
    "counts = (10 * np.random.choice(p, size=num_samples, replace=True, p=post)).round()\n",
    "plt.hist(counts, bins=int(max(counts)))\n",
    "plt.title('Probability vs. number of distracted drivers in next %d' % num_cars)\n",
    "plt.xlabel('Number of distracted drivers')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Bayesian models\n",
    "\n",
    "How can we use Bayesian models to compare two distributions? It turns out that we can compare Bayesian models in several ways. In this lesson, we will compute and compare confidence intervals of the posterior distribution of a model parameter. \n",
    "\n",
    "For this example, we will compare the posterior distribution of the heights of sons to the heights of the mothers in the Galton family dataset. As a first step, we will compute and evaluate Bayesian models for the mean heights using a subset of just 25 observations. \n",
    "\n",
    "The code in the cell below sub-samples the Galton family data and then plots histogram of the heights of sons and mothers. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/GaltonFamilies.csv'\n",
    "families = pd.read_csv(file_name, index_col=0)\n",
    "families.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "male = families[families.gender == 'male'].sample(n=num_samples)\n",
    "\n",
    "plt.title('Histograms of heights of people')\n",
    "male.childHeight.hist(label='sons', bins=10, alpha=.7)\n",
    "male.mother.hist(label='mothers', bins=10, alpha=.7)\n",
    "plt.xlabel('Height')\n",
    "plt.legend()\n",
    "\n",
    "mean_height = np.concatenate([male.mother, male.father, male.childHeight,]).mean()\n",
    "print('Mean of heights: %.1f' % mean_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this analysis, we need to select a prior distribution and compute the likelihood. First, we will address the likelihood. \n",
    "\n",
    "For these data, we will use a Normal likelihood. For a sample $X = {x_1, x_2, \\ldots, x_n}$, we can write the likelihood as:\n",
    "\n",
    "$$\n",
    "P(X | u, \\sigma) = \\bigg(\\frac{1}{2 \\pi \\sigma^2} \\bigg)^{\\frac{n}{2}} exp \\Bigg[ -\\frac{1}{2 \\sigma^2}  \\Bigg( \\sum_{i = 1}^n (x_i - \\bar{x})^2 + n(\\bar{x} - \\mu)^2 \\Bigg) \\Bigg] \\\\\n",
    "ignoring\\ constants\\ and\\ normalization\\\\\n",
    "\\propto exp \\bigg( -\\frac{n(\\bar{x} - \\mu)^2}{2 \\sigma^2} \\bigg) \n",
    "$$\n",
    "\n",
    "To simplify the computations here, we will only estimate the posterior distribution of the mean. We will use a fixed empirical estimate of the standard deviation. A more complete analysis will also estimate the posterior distribution of the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "p = np.linspace(60, 75, num=N)\n",
    "\n",
    "pp = scipy.stats.norm.pdf(p, loc=male.childHeight.mean(), scale=5)\n",
    "pp = pp / pp.sum() # normalize\n",
    "\n",
    "def comp_like(p, x): \n",
    "    variance = np.std(x)**2 # sigmasqr\n",
    "    x_mean = np.asarray(x).mean()  # xbar\n",
    "    print('Mean = %.3f, Standard deviation = %.3f' % (x_mean, np.std(x)))\n",
    "    n = len(x)\n",
    "    l = np.exp(-n * np.square(x_mean - p) / (2 * variance))\n",
    "    return l / l.sum()\n",
    "\n",
    "like_son = comp_like(p, male.childHeight)\n",
    "post_son = posterior(pp, like_son)\n",
    "\n",
    "plt.plot(p, pp, label='prior')\n",
    "plt.plot(p, like_son, label='likelihood', alpha=.3, linewidth=10)\n",
    "plt.plot(p, post_son, label='posterior')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes the posterior distribution of the heights of the mothers. Run this code and examine the results. How do these results differ from the results for the heights of the sons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_mom = scipy.stats.norm.pdf(p, loc=male.mother.mean(), scale=5)\n",
    "pp_mom = pp_mom / pp_mom.sum() # normalize\n",
    "\n",
    "like_mom = comp_like(p, male.mother)\n",
    "post_mom = posterior(pp_mom, like_mom)\n",
    "\n",
    "plt.plot(p, pp_mom, label='prior')\n",
    "plt.plot(p, like_mom, label='likelihood', alpha=.3, linewidth=10)\n",
    "plt.plot(p, post_mom, label='posterior')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the posterior distributions of the mean heights of the sons to the distribution of the mean heights of the mothers, we compute and compare the confidence intervals. \n",
    "\n",
    "Run the code  in the cell below which computes and plots the confidence intervals for the mean heights of the sons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "\n",
    "plot_ci(p, post_son, num_samples, lower_q=.025, upper_q=.975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute and plot the posterior distribution and CIs of the mean of the heights of the mothers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ci(p, post_mom, num_samples, lower_q=.025, upper_q=.975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the CIs for these posterior distributions. Are the distributions of the mean heights of sons and mothers significantly different?  \n",
    "\n",
    "***\n",
    "## Your Turn 2\n",
    "Try the variations on the foregoing Bayesian analysis.\n",
    "\n",
    "1. The foregoing Bayesian analysis was performed with just 25 data points. Rerun this analysis with with 250 data points. How do the posterior distributions computed with the 250 data points compare to those computed with 25 data points? \n",
    "2. Perform the same analysis, except comparing the distributions of heights of sons and heights of fathers. \n",
    "\n",
    "**Important!:** Use another variable name for the likelihood and posterior for this exercise. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the other side of the =\n",
    "# use 250 data points\n",
    "num_samples = \n",
    "male_samples = \n",
    "\n",
    "like_son_250 = \n",
    "post_son_250 = \n",
    "\n",
    "like_mom_250 = \n",
    "post_mom_250 = \n",
    "\n",
    "# Plotting the Posterior distributions and CIs\n",
    "plot_ci(p, post_son_250, num_samples, lower_q=.025, upper_q=.975)\n",
    "plot_ci(p, post_mom_250, num_samples, lower_q=.025, upper_q=.975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation from the Posterior Distribution\n",
    "\n",
    "Once we have a posterior distribution for parameters we can simulate from this distribution. The simulation consists of taking a number of random draws from the posterior parameter distribution and computing the posterior distribution of the data values. \n",
    "\n",
    "There are a number of reasons why you might want to simulate from the posterior distribution of data values.\n",
    "\n",
    "- Test the model against the data.\n",
    "- Compute forecasts of the dependent (label) variable from the model.\n",
    "\n",
    "The code in the cell below computes the posterior distribution of the heights of sons. This is done by computing the distribution of height based on realizations of the parameter (the mean) from the posterior distribution. The density distribution of the simulated heights is plotted along with the histogram of the original data. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_height(n, sigma, p, post):\n",
    "    # create the probability-weighted random sample of values of the mean height\n",
    "    mu = np.random.choice(p, size=n, replace=True, p=post)\n",
    "    return scipy.stats.norm.rvs(loc=mu, scale=sigma, size=n)\n",
    "\n",
    "plt.hist(sim_height(10000, sigma=2.616329, p=p, post=post_son))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(n, post, dat):\n",
    "    plt.hist(dat, density=True, label='data')\n",
    "    sns.kdeplot(post, label='posterior sample distribution')\n",
    "    plt.title('Histogram of data with density of model predictions')\n",
    "\n",
    "sim_vals = sim_height(10000, sigma=2.616329, p=p, post=post_son)\n",
    "plot_dist(10000, sim_vals, male.childHeight)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this chart. How well does the density of the posterior value distribution match the histogram of the original data values? Does the density of the posterior value distribution deviate from normal? \n",
    "\n",
    "***\n",
    "## Your Turn 3\n",
    "Use the model you computed with 250 data values to create a plot like the one above. Examine these results and compare them to the results obtained with the model created with only 25 data values. Which posterior density function appears to represent the data better and why? Does the density deviate from normal?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the other side of the =\n",
    "sim_vals_250 = \n",
    "\n",
    "# Plot the distribution\n",
    "plot_dist(10000, sim_vals_250, male.childHeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, you have explored the following concepts:\n",
    "\n",
    "1. Application of Bayes Theorem.\n",
    "2. Computation of marginal distributions.\n",
    "3. Selection and computation of prior distributions.\n",
    "4. Selection and computation of likelihoods.\n",
    "5. Computation of posterior distributions.\n",
    "6. Computation and comparison of credible intervals. \n",
    "7. Simulation of data values from posterior distribution of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "342.367px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
