{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to text analytics\n",
    "\n",
    "This notebook contains a tutorial introduction to basic text analytics with Python.  Raw text data is an unstructured and ubiquitous type of data. Most of the world’s data is unstructured. Volumes of unstructured data, including text, are growing much faster than structured data. There are many industry estimates for the fraction of all data which is unstructured. How much text data are we talking about here? In a few years time, Twitter has [more text data recorded](http://www.internetlivestats.com/twitter-statistics/) than all that has been written in print in the history of mankind.\n",
    "\n",
    "Given the ubiquity and volume of text data, it is not surprising that numerous powerful applications which exploit text analytics are appearing. A few of these applications are listed below.\n",
    "\n",
    "- Intelligent applications\n",
    "  - Assistants\n",
    "  - Chat bots\n",
    "- Classification\n",
    "  - Sentiment analysis\n",
    "  - SPAM detection\n",
    "- Speech recognition\n",
    "- Search\n",
    "- Information retrieval\n",
    "- Legal discovery\n",
    "\n",
    "In this tutorial we investigate three areas of text analytics. The following three sections cover these topics.\n",
    "\n",
    "- Preparing text for analysis\n",
    "- Classification of text and sentiment analysis\n",
    "- Topic Models for document classification and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install editdistance\n",
    "# !pip install nltk\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'editdistance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-10e7bc2c01ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0meditdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'editdistance'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import editdistance\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "import argparse\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import Counter\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sethmott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sethmott\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# If you get an SSL-certificate error, and you are on a MAC then you may have to navigate to: application/python3/ and\n",
    "# run/double-click on the command 'install certificates'.  Then try this again.\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By its very nature, text data comes unstructured and poorly organized for analysis. Typically multiple steps are required to process text into a form suitable for analysis. You can think of this process as transforming the unstructured data into a structured set of features.\n",
    "\n",
    "There are many more approaches for text analytics and natural language processing (NLP). We only mention a few below. Note that the collection of unique words in the data is called a **corpus**. To avoid having a corpus that's too large, we can trim the corpus by keeping the most frequent $N$ words, making $N$ the size of the corpus. A **document** usually refers to a single data point with raw text, such as a tweet, a review, an invoice, etc.\n",
    "\n",
    "- The **bag of words model** is a simple widely used and surprisingly effective model for analysis of text data. The BOW model creates a **sparse vector representation** of each word in the corpus based on the frequency of the words in the document. The order of the words is not considered, nor is the similarity between different words. Despite serious shortcomings, the model can work well in many cases. \n",
    "- We can usually do much better by using **word embeddings**, which are **dense vector respresentations** for each word in the corpus. Word embeddings are learned by examining the word's **context** (other words around it). Word embeddings are very common in deep learning applications of NLP, although the embeddings themselves are learned using a shallow network.\n",
    "- Another widely used model is of **part-of-speech (PoS) tagging**. PoS tagging attempts to label or annotate words in a corpus as nouns, verbs, pronouns, etc, and creates a tree of relationships between the words in the document. PoS tagging is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying text\n",
    "\n",
    "Whether we have numeric or text data, there are many distance metrics we can use, and the choice of the metric we use can affect the results we get when using algorithms that rely on distances between data points, such as k-means clustering, SVMs, etc. Note that by \"data points\" we are referring to two features represented as **vectors**. The most common distance metric is the **Euclidean distance**, but there are many other choices such as **Manhattan distance** or **cosine similarity** (similarity is the inverse of distance). There are also distance metrics we can use specifically for words, if we represent each word as a vector whose $i$th position is filled by the word's $i$th letter. Here are some examples:\n",
    "\n",
    "- The **Hamming distance** lines up strings and counts the number of positions that are the different. The strings must have the same length. For example, the Hamming distance between \"bear\" and \"beer\" is 1.\n",
    "- The **Levenshtein distance** (also called the **edit distance**), measure the distance between two strings of variable length by counting the number of insertions, deletions, substitutions needed to convert one string into the other. For example, the Levenshtein distance between \"banana\" and \"ban\" is 3, because 3 insertions are needed. In Levenshtein distance specifically, substitutions have a cost of 2, while insertions and deletions have a cost of 1 associated with each.\n",
    "- The **Jaccard index** measures the size of intersection of characters divided by size of union of characters.\n",
    "\n",
    "  $$J(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "  For example, $J(\\text{beer}, \\text{bear}) = 1 - \\frac{3}{4}$. Note that if we do not subtract the ratio from one, we get the Jaccard **similarity**. So the distance is one minus the similarity (and vice versa).\n",
    "- The **weighted Jaccard index** is based on the minimum number of times $m_i$ that a letter appears and the maximum number of times $M_i$ it appears in both words **combined**. We then measure their distance as such:\n",
    "\n",
    "  $$J'(A, B) = 1 - \\frac{\\sum m_i}{\\sum M_i}$$\n",
    "\n",
    "  For example, $J'(\\text{beer}, \\text{bear}) = 1 - \\frac{m_a + m_e + m_b + m_r}{M_a + M_e + M_b + M_r} = 1 - \\frac{0 + 1 + 1 + 1}{1 + 2 + 1 + 1} = 1 - \\frac{3}{5}$.\n",
    "\n",
    "We can try to implement these distances with Python, but this would be more akin to a programming exercise, so instead we call functions from third-party libraries when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and preprocessing\n",
    "\n",
    "The very first step to prepare text is to clean it.  We clean and normalize the text by performing various operations on the text. Some examples are as follows:\n",
    "\n",
    "- Make the text lowercase.\n",
    "- Remove symbols or punctuation.\n",
    "- Remove numbers. May also replace all numbers with a numeric tag, for example `<NUM>` or similar. We may also consider replacing all dates with `<DATE>` or similarly use tags `<URL>`, `<PHONE>`, `<EMAIL>`, etc...\n",
    "- Strip extra white space. White space has many forms: space, newline, or tab. There are also other rarely used unicode specifications for other white space characters.\n",
    "- Remove all non-printable unicode characters.\n",
    "- Replace accent characters.\n",
    "- Remove 'stop words'. Stop words are generally non-informative words like \"the\", \"as\", \"a\", etc.\n",
    "- Stem words to similar endings, such as \"eats\" and \"eat\".\n",
    "\n",
    "There are a few reasons to clean your text.  The primary reason is to reduce the potential vocabulary and increase the observations of specific words (or tokens). Depending on the application, the above steps should be considered carefully and only applied when it makes sense. Ask yourself if words like \"China\" and \"china\" to be different.\n",
    "\n",
    "**NOTE**: Be careful dealing with unicode characters. There are many editors and text viewers that only display printable characters but will not remove non-printable characters. Strange unicode characters can end up in data from users blindly copy/pasting text (with invisible unicode) into other text boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Here's a horrible tweet. We're going to clean it and learn about ways to clean text data in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered text: \n",
      "I <3 statistics $\\ \\ $, it’s my ၲ  $\\ \\ $    fAvoRitE!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horrible_tweet_text = 'I <3 statistics $\\ \\ $, it’s my \\u1072  $\\ \\ $    fAvoRitE!! 11!!!'\n",
    "print('Unfiltered text: \\n{}\\n'.format(horrible_tweet_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step, refine the above text. To avoid overwriting the existing string, you can create a new string from the string so far each time.\n",
    "\n",
    "- Remove any non-ASCI character. A string `x` is ASCI if `ord(x) < 128`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make all the letters lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all punctuation from the text. You can use `string.punctuation` to get a string of all punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the numbers from the text. HINT: If you are familiar with **regular expressions** you can use `re.sub` to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strip the text of any extra whitespace. HINT: The `split` method might help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all stop words from the text. We can use `stopwords.words('english')` to get a list of stop words for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's now use `WordNetLemmatizer()` to reduce the words to their **stems**. HINT: Use the object's `lemmatize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine the steps outlined in the above exercise and create a function to clean text for use. We will later use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: \"I <3 statistics $\\ \\ $, it’s my ၲ  $\\ \\ $    fAvoRitE!! 11!!!\"\n",
      "after : \"statistic favorite\"\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_stopwords':\n",
    "            stops = stopwords.words('english')\n",
    "            word_list = text.split(' ')\n",
    "            text_words = [word for word in word_list if word not in stops]\n",
    "            text = ' '.join(text_words)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "step_list = ['remove_non_ascii', 'lowercase', 'remove_punctuation', 'remove_numbers',\n",
    "            'strip_whitespace', 'remove_stopwords', 'stem_words']\n",
    "\n",
    "print(\"before: \\\"{}\\\"\".format(horrible_tweet_text))\n",
    "print(\"after : \\\"{}\\\"\".format(preprocess(horrible_tweet_text, step_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preparation\n",
    "Unstructured text must be processed into a uniform set of features suitable for further analysis. In this section we will step through some of the commonly used methods for converting unstructured text into a form we can use for analysis. \n",
    "\n",
    "### Tokenize text\n",
    "\n",
    "As a first step in preparing text for analysis of a document is to **tokenize** the text. In general terms, tokenization is the process dividing raw text into words, symbols and other elements, known as **tokens**. A set of tokens from all documents in the data is known as a **corpus**.\n",
    "\n",
    "As a first step in creating a corpus is reading the data set. This particular data set is comprised of 160,000 tweets. The sentiment of these tweets has been human labeled as positive or negative (4 is for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text\n",
       "0                4  @elephantbird Hey dear, Happy Friday to You  A...\n",
       "1                4  Ughhh layin downnnn    Waiting for zeina to co...\n",
       "2                0  @greeniebach I reckon he'll play, even if he's...\n",
       "3                0              @vaLewee I know!  Saw it on the news!\n",
       "4                0  very sad that http://www.fabchannel.com/ has c..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/twitter_data.csv'\n",
    "tweet_df = pd.read_csv(data_file)\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    80000\n",
       "0    80000\n",
       "Name: sentiment_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['sentiment_label'] = tweet_df['sentiment_label'].replace(4, 1)\n",
    "tweet_df['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = tweet_df.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data set read, we need to clean then tokenize the tweets. Note that stemming can be slow on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace', 'stem_words']\n",
    "tweet_df['clean_tweet'] = tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "      <td>elephantbird hey dear happy friday to you alre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "      <td>ughhh layin downnnn waiting for zeina to cook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "      <td>greeniebach i reckon hell play even if he not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "      <td>valewee i know saw it on the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "      <td>very sad that httpwwwfabchannelcom ha closed d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text  \\\n",
       "0                1  @elephantbird Hey dear, Happy Friday to You  A...   \n",
       "1                1  Ughhh layin downnnn    Waiting for zeina to co...   \n",
       "2                0  @greeniebach I reckon he'll play, even if he's...   \n",
       "3                0              @vaLewee I know!  Saw it on the news!   \n",
       "4                0  very sad that http://www.fabchannel.com/ has c...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  elephantbird hey dear happy friday to you alre...  \n",
       "1  ughhh layin downnnn waiting for zeina to cook ...  \n",
       "2  greeniebach i reckon hell play even if he not ...  \n",
       "3                  valewee i know saw it on the news  \n",
       "4  very sad that httpwwwfabchannelcom ha closed d...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's sometimes helpful to profile such functions to find the main culprits and see if we can do anything to speed them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         46070046 function calls (46070043 primitive calls) in 55.299 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(copyto)\n",
      "        4    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:997(_handle_fromlist)\n",
      "   160000    2.868    0.000   54.424    0.000 <ipython-input-5-0f7b3a1f02b1>:1(preprocess)\n",
      " 11257197    4.353    0.000    4.353    0.000 <ipython-input-5-0f7b3a1f02b1>:10(<genexpr>)\n",
      "   160000    1.986    0.000   40.170    0.000 <ipython-input-5-0f7b3a1f02b1>:23(<listcomp>)\n",
      "   160000    0.599    0.000   55.023    0.000 <string>:1(<lambda>)\n",
      "        1    0.010    0.010   55.299   55.299 <string>:1(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 _dtype.py:319(_name_includes_bit_suffix)\n",
      "        2    0.000    0.000    0.000    0.000 _dtype.py:333(_name_get)\n",
      "        2    0.000    0.000    0.000    0.000 _dtype.py:36(_kind_name)\n",
      "        2    0.000    0.000    0.000    0.000 _internal.py:830(npy_ctypes_check)\n",
      "        7    0.000    0.000    0.000    0.000 _weakrefset.py:70(__contains__)\n",
      "        5    0.000    0.000    0.000    0.000 abc.py:180(__instancecheck__)\n",
      "        1    0.000    0.000   55.282   55.282 base.py:1073(_map_values)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1383(nlevels)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1656(is_unique)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:247(is_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:2637(get_loc)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3898(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:3912(__getitem__)\n",
      "        3    0.000    0.000    0.000    0.000 base.py:5294(ensure_index)\n",
      "        2    0.000    0.000    0.000    0.000 base.py:5394(maybe_extract_name)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:118(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:129(_check_ndim)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:187(is_categorical_astype)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:202(external_values)\n",
      "        4    0.000    0.000    0.000    0.000 blocks.py:247(mgr_locs)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:251(mgr_locs)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:2585(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:275(make_block_same_class)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:2975(get_block_type)\n",
      "        3    0.000    0.000    0.000    0.000 blocks.py:3021(make_block)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:3047(_extend_blocks)\n",
      "        2    0.000    0.000    0.000    0.000 blocks.py:339(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 blocks.py:365(iget)\n",
      "        1    0.000    0.000    0.003    0.003 blocks.py:554(astype)\n",
      "        1    0.000    0.000    0.003    0.003 blocks.py:694(copy)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1097(maybe_castable)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1111(maybe_infer_to_datetimelike)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1218(maybe_cast_to_datetime)\n",
      "        1    0.000    0.000    0.000    0.000 cast.py:1492(construct_1d_ndarray_preserving_na)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:156(cast_scalar_indexer)\n",
      "        6    0.000    0.000    0.000    0.000 common.py:1565(is_extension_array_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1672(_get_dtype)\n",
      "        5    0.000    0.000    0.000    0.000 common.py:1708(_is_dtype_type)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:1844(pandas_dtype)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:206(classes)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:208(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:211(classes_and_not_datetimelike)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:216(<lambda>)\n",
      "        4    0.000    0.000    0.000    0.000 common.py:222(is_object_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:252(is_sparse)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:335(apply_if_callable)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:339(is_categorical)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:403(is_datetime64tz_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:472(is_period_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:506(is_interval_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 common.py:542(is_categorical_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:685(is_dtype_equal)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:775(is_integer_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:337(extract_array)\n",
      "        1    0.000    0.000    0.005    0.005 construction.py:388(sanitize_array)\n",
      "        1    0.000    0.000    0.000    0.000 construction.py:506(_try_cast)\n",
      "        2    0.000    0.000    0.000    0.000 construction.py:570(is_empty_data)\n",
      "        1    0.000    0.000    0.000    0.000 dtypes.py:1124(is_dtype)\n",
      "        7    0.000    0.000    0.000    0.000 dtypes.py:75(find)\n",
      "        1    0.000    0.000    0.000    0.000 dtypes.py:917(is_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:2767(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:3066(_box_item_values)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:3073(_box_col_values)\n",
      "       31    0.000    0.000    0.000    0.000 generic.py:10(_check)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:190(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:238(attrs)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3227(_set_as_cached)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:3581(_get_item_cache)\n",
      "        2    0.000    0.000    0.000    0.000 generic.py:5235(__finalize__)\n",
      "        3    0.000    0.000    0.000    0.000 generic.py:5257(__getattr__)\n",
      "        5    0.000    0.000    0.000    0.000 generic.py:5276(__setattr__)\n",
      "        1    0.000    0.000    0.003    0.003 generic.py:5563(astype)\n",
      "        3    0.000    0.000    0.000    0.000 inference.py:299(is_dict_like)\n",
      "        6    0.000    0.000    0.000    0.000 inference.py:325(<genexpr>)\n",
      "        9    0.000    0.000    0.000    0.000 inference.py:358(is_hashable)\n",
      "        1    0.000    0.000    0.000    0.000 inspect.py:73(isclass)\n",
      "        3    0.000    0.000    0.000    0.000 managers.py:1467(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:1520(_block)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1544(index)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1548(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1562(external_values)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:1582(_consolidate_inplace)\n",
      "        2    0.000    0.000    0.000    0.000 managers.py:232(items)\n",
      "        1    0.000    0.000    0.003    0.003 managers.py:368(apply)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:419(<dictcomp>)\n",
      "        1    0.000    0.000    0.003    0.003 managers.py:581(astype)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:950(get)\n",
      "        1    0.000    0.000    0.000    0.000 managers.py:979(iget)\n",
      "        1    0.000    0.000    0.000    0.000 missing.py:132(_isna_new)\n",
      "        1    0.000    0.000    0.000    0.000 missing.py:49(isna)\n",
      "        1    0.000    0.000    0.000    0.000 multiarray.py:1043(copyto)\n",
      "        3    0.000    0.000    0.000    0.000 numeric.py:155(is_all_dates)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:283(full)\n",
      "        3    0.000    0.000    0.000    0.000 range.py:675(__len__)\n",
      "   160000    0.303    0.000    2.059    0.000 re.py:184(sub)\n",
      "   160000    0.324    0.000    0.324    0.000 re.py:286(_compile)\n",
      "        3    0.000    0.000    0.006    0.002 series.py:183(__init__)\n",
      "        1    0.002    0.002   55.290   55.290 series.py:3555(map)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:359(_constructor)\n",
      "        3    0.000    0.000    0.000    0.000 series.py:376(_set_axis)\n",
      "        3    0.000    0.000    0.000    0.000 series.py:403(_set_subtyp)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:414(dtype)\n",
      "        5    0.000    0.000    0.000    0.000 series.py:428(name)\n",
      "        5    0.000    0.000    0.000    0.000 series.py:432(name)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:438(values)\n",
      "  2070416    6.944    0.000   32.698    0.000 wordnet.py:1865(_morphy)\n",
      "  2092191    2.306    0.000   19.891    0.000 wordnet.py:1876(apply_rules)\n",
      "  2092191    9.155    0.000   17.585    0.000 wordnet.py:1878(<listcomp>)\n",
      "  2119667    5.241    0.000    5.863    0.000 wordnet.py:1884(filter_forms)\n",
      "   160000    0.059    0.000    0.059    0.000 wordnet.py:34(__init__)\n",
      "  2070416    3.965    0.000   38.183    0.000 wordnet.py:37(lemmatize)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "        1    0.000    0.000   55.299   55.299 {built-in method builtins.exec}\n",
      "       48    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "       90    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "       25    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "185226/185223    0.051    0.000    0.051    0.000 {built-in method builtins.len}\n",
      "  1069232    1.520    0.000    1.520    0.000 {built-in method builtins.min}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method pandas._libs.missing.checknull}\n",
      "  1092023    0.275    0.000    0.275    0.000 {method 'add' of 'set' objects}\n",
      "  1092024    0.347    0.000    0.347    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.003    0.003    0.003    0.003 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      " 18848997    8.379    0.000    8.379    0.000 {method 'endswith' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'get_loc' of 'pandas._libs.index.IndexEngine' objects}\n",
      "   480000    3.959    0.000    8.312    0.000 {method 'join' of 'str' objects}\n",
      "   160000    0.108    0.000    0.108    0.000 {method 'lower' of 'str' objects}\n",
      "   320000    0.849    0.000    0.849    0.000 {method 'split' of 'str' objects}\n",
      "   160000    1.432    0.000    1.432    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.algos.ensure_object}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.infer_datetimelike_array}\n",
      "        1    0.005    0.005    0.005    0.005 {pandas._libs.lib.infer_dtype}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_float}\n",
      "        5    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_list_like}\n",
      "        2    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_scalar}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.item_from_zerodim}\n",
      "        1    0.255    0.255   55.279   55.279 {pandas._libs.lib.map_infer}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "cProfile.run(\"tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term document matrix\n",
    "\n",
    "Now that we have a corpus with some basic normalization applied, we can create a **term document matrix (TDM)**. The TDM is a representation for a **bag of words** model. The TDM is a sparse matrix, as most documents do not include many of the terms. Sparse matrix coding must be used for efficiency. The rows represent the terms and the columns are the documents, so the entries show the frequency of each term for each document.\n",
    "\n",
    "![](https://library.startlearninglabs.uw.edu/DATASCI410/img/tdm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute this for an example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12 distinct words.\n",
      "\n",
      "['i', 'fun', 'machine', 'machines', 'think', 'much', 'learn', 'coding', 'learning', 'can', 'so', 'is']\n",
      "[[0 1 0 1]\n",
      " [1 1 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 1 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "example_texts = [\n",
    "    'machine learning is so much fun',\n",
    "    'i think learning is fun',\n",
    "    'machines can learn',\n",
    "    'i think coding is fun'\n",
    "]\n",
    "\n",
    "vocab = set() # create the vocabulary\n",
    "for text in example_texts:\n",
    "    words = text.split(' ')\n",
    "    vocab.update(set(words))\n",
    "\n",
    "vocab_list = list(vocab)\n",
    "print('Vocabulary Size: {} distinct words.\\n'.format(len(vocab_list)))\n",
    "\n",
    "# create term-document matrix\n",
    "d_t_matrix = np.zeros((len(vocab), len(example_texts)), dtype = np.intc)\n",
    "for doc_ix_col, text in enumerate(example_texts):\n",
    "    text_words = text.split(' ')\n",
    "    row_ixs = [vocab_list.index(word) for word in text_words if word in vocab_list]\n",
    "    d_t_matrix[row_ixs, doc_ix_col] = 1\n",
    "\n",
    "print(vocab_list)\n",
    "print(d_t_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that with a large corpus, our term-document matrix will get very sparse very quickly. As a sparse matrix, we store the occurrences with coordinates and values. This type of sparse matrix is called a **COO matrix** or a coordinate matrix.\n",
    "\n",
    "Let's see how to do this for the tweet-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry: ['pyoorgrant', 'i', 'totally', 'will', 'really', 'i', 'neeeed', 'money']\n"
     ]
    }
   ],
   "source": [
    "clean_texts = tweet_df['clean_tweet']\n",
    "docs = {}\n",
    "labels = []\n",
    "for ix, row in enumerate(clean_texts):\n",
    "    labels = tweet_data[ix][0]\n",
    "    docs[ix] = row.split(' ')\n",
    "\n",
    "print('Example entry: {}'.format(docs[np.random.choice(ix)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep track of how many unique words there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our tweet-vocabulary has 146496 distinct words.\n"
     ]
    }
   ],
   "source": [
    "num_nonzero = 0\n",
    "vocab = set()\n",
    "\n",
    "for word_list in docs.values():\n",
    "    unique_terms = set(word_list)    # all unique terms of this tweet\n",
    "    vocab.update(unique_terms)       # set union: add unique terms of this tweet\n",
    "    num_nonzero += len(unique_terms) # add count of unique terms in this tweet\n",
    "\n",
    "doc_key_list = list(docs.keys())\n",
    "\n",
    "print('Our tweet-vocabulary has {} distinct words.'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert everything to a numpy array. We should keep track of how the vocab/term indices map to the matrix so that we can look them up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['mccurdy' 'area' 'skipd' 'watt' 'levelchick']\n",
      "Sorted Vocab: ['a' 'aa' 'aaa' 'aaaa' 'aaaaa']\n"
     ]
    }
   ],
   "source": [
    "doc_key_list = np.array(doc_key_list)\n",
    "vocab = np.array(list(vocab))\n",
    "\n",
    "vocab_sorter = np.argsort(vocab)\n",
    "\n",
    "print('Vocab: {}'.format(vocab[:5]))\n",
    "print('Sorted Vocab: {}'.format(vocab[vocab_sorter[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A COO matrix is just a tuple of data, row indices, and column indices. Everything else is assumed to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(doc_key_list)\n",
    "vocab_size = len(vocab)\n",
    "data = np.empty(num_nonzero, dtype = np.intc)     # all non-zero\n",
    "rows = np.empty(num_nonzero, dtype = np.intc)     # row index\n",
    "cols = np.empty(num_nonzero, dtype = np.intc)     # column index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to compute the full term-document sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing, please wait!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ix = 0\n",
    "# go through all documents with their terms\n",
    "print('Computing, please wait!')\n",
    "for doc_key, terms in docs.items():\n",
    "    # find indices to insert-into such that, if the corresponding elements were\n",
    "    # inserted before the indices, the order would be preserved\n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "    n_vals = len(uniq_indices)  # = number of unique terms\n",
    "    ix_end = ix + n_vals # Add count to index.\n",
    "\n",
    "    data[ix:ix_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ix:ix_end] = uniq_indices            # save the column index: index in \n",
    "    doc_ix = np.where(doc_key_list == doc_key)   # get the document index for the document name\n",
    "    rows[ix:ix_end] = np.repeat(doc_ix, n_vals)  # save it as repeated value\n",
    "\n",
    "    ix = ix_end  # resume with next document -> will add future data on the end.\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our sorted vocabulary again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five terms alphabetically: ['a' 'aa' 'aaa' 'aaaa' 'aaaaa']\n"
     ]
    }
   ],
   "source": [
    "print('First five terms alphabetically: {}'.format(vocab[vocab_sorter[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we probably need to do some trimming, as the word 'aaaaa' probably doesn't occur often enough, and having 151,670 unique words may be too much.  We will address this later on. For now, let's create the sparse coordinate matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_mat = coo_matrix((data, (rows, cols)), shape = (num_docs, vocab_size), dtype = np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab index of math : 106035\n",
      "\n",
      "1st document index containing said word: 171\n",
      "\n",
      "Tweet: [1, 'yayyy, i miss out on science and math tomoro!!  but then i have french in the morning. eugh, fair trade =__=']\n",
      "\n",
      "Document-Term Matrix entry: 1\n"
     ]
    }
   ],
   "source": [
    "# let's check to make sure!\n",
    "vocab_list = list(vocab)\n",
    "word_of_interest = 'math'\n",
    "vocab_interesting_ix = list(vocab).index(word_of_interest)\n",
    "print('vocab index of {} : {}'.format(word_of_interest, vocab_interesting_ix))\n",
    "# find which tweets contain word\n",
    "doc_ix_with_word = []\n",
    "for ix, row in enumerate(tweet_data): # note on this line later\n",
    "    if word_of_interest in row[1]:\n",
    "        doc_ix_with_word.append(ix)\n",
    "\n",
    "print('\\n1st document index containing said word: {}'.format(doc_ix_with_word[0]))\n",
    "print('\\nTweet: {}'.format(tweet_data[doc_ix_with_word[0]]))\n",
    "\n",
    "# document - term matrix relevant entry\n",
    "document_row = doc_ix_with_word[0]\n",
    "vocab_col = vocab_interesting_ix\n",
    "mat_entry = doc_term_mat.tocsr()[document_row, vocab_col]\n",
    "\n",
    "print('\\nDocument-Term Matrix entry: {}'.format(mat_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to be careful with our usage of test-words and python's `in` function.  Imagine we have a tweet that has punctuation or the word is a sub-sequence of another word, then the `in` function would tag the document as true, but our cleaning/parsing would not have that as a separate word.\n",
    "\n",
    "You can see for yourself what is happening if you use the test word \"python\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming the document-term matrix\n",
    "\n",
    "We saw above that we are including terms like 'aaaaa' and 'aaaa', which probably occur very few times. These terms generally occur with unstructured text fields because we allow users to input whatever they feel like and that includes typos.  But be aware that they can also be artifacts of our cleaning process (unintentionally and intentionally).\n",
    "\n",
    "Since our document-term matrix is a matrix of counts of words (columns) in each document (rows), we want to remove words that don't occur very frequently across our corpus. The count of how frequent a word is in all of our corpus is just the sum of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 134   1 ...   1   1   2]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = doc_term_mat.sum(axis = 0)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many words are above a specific cutoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words w/counts above 15 : 5856\n"
     ]
    }
   ],
   "source": [
    "cutoff = 15\n",
    "word_count_list = word_counts.tolist()[0]\n",
    "col_cutoff_ix = [ix for ix, count in enumerate(word_count_list) if count > cutoff]\n",
    "\n",
    "print('Number of words w/counts above {} : {}'.format(cutoff, len(col_cutoff_ix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now trim our vocabulary and document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix before trimming: (160000, 146496)\n",
      "Shape of document-term matrix after trimming: (160000, 5856)\n"
     ]
    }
   ],
   "source": [
    "vocab_trimmed = np.array([vocab[x] for x in col_cutoff_ix])\n",
    "vocab_sorter_trimmed = np.argsort(vocab_trimmed)\n",
    "\n",
    "print('Shape of document-term matrix before trimming: {}'.format(doc_term_mat.shape))\n",
    "\n",
    "doc_term_mat_trimmed = doc_term_mat.tocsc()[:,col_cutoff_ix]\n",
    "print('Shape of document-term matrix after trimming: {}'.format(doc_term_mat_trimmed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'aa', 'aaa', 'aaaah', 'aaah', 'aah', 'aaron', 'ab',\n",
       "       'abandoned', 'abby'], dtype='<U37')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first 10 words alphabetically\n",
    "vocab_trimmed[vocab_sorter_trimmed[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what cutoff we should use? Let's look at a bar graph of frequency of words before and after we trimmed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       0.749986\n",
       "2       0.099764\n",
       "3       0.037080\n",
       "4       0.019652\n",
       "5       0.012581\n",
       "          ...   \n",
       "837     0.000007\n",
       "453     0.000007\n",
       "197     0.000007\n",
       "3013    0.000007\n",
       "315     0.000007\n",
       "Length: 853, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(word_count_list, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many words that appear few times. Let's check out the trimmed list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16      0.040813\n",
       "17      0.040471\n",
       "18      0.033470\n",
       "19      0.030396\n",
       "21      0.028176\n",
       "          ...   \n",
       "2459    0.000171\n",
       "410     0.000171\n",
       "398     0.000171\n",
       "390     0.000171\n",
       "1945    0.000171\n",
       "Length: 838, dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_word_counts = doc_term_mat_trimmed.sum(axis = 0)\n",
    "trimmed_word_list = trimmed_word_counts.tolist()[0]\n",
    "pd.value_counts(trimmed_word_list, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 15 might be a good cutoff because we get a visible drop off and for our next model, ~6000 features may not be too much.  These types of **hyperparameters** for the model will probably need tuning.\n",
    "\n",
    "The best way to find the cutoff is to test various model preprocessing through a train-test-validation split.  Train multiple versions and compare them on the test set.  Then report the best resulting model's performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF-IDF\n",
    "\n",
    "Now that we have computed a document-term matrix, how can we understand it? Recall that the simple **bag of words model** is just based on **term frequency (TF)** where $\\text{TF(term, doc)}$ is the frequency of some term in a given document.\n",
    "\n",
    "We can also derive the **inverse document frequencies (IDF)**:\n",
    "\n",
    "$$\\text{IDF(term)} = \\log(\\frac{\\text{number of documents}}{\\text{number of documents with term in it} + 1})$$\n",
    "\n",
    "Here's what the IDF function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAdOklEQVR4nO3deXBc1YHv8e/R5tba2nfJki153xG2sVmMCcFsk40QSCYh2zgTskAqb1JJJe9lpmYqL5M3RYDJQggQkoEACVsY1iHYrAbb8oJlI3mXLMmyFmvflz7vj24rArzItlr3qvv3qeqSutW2freu+dXh9Dn3GmstIiLiXhFOBxARkdNTUYuIuJyKWkTE5VTUIiIup6IWEXG5qGD8penp6baoqCgYf7WISEjatm1bi7U242Q/C0pRFxUVUV5eHoy/WkQkJBljak71M019iIi4nIpaRMTlVNQiIi6nohYRcTkVtYiIy6moRURcTkUtIuJyrilqn8/yiw37eW1fs9NRRERcxTVFHRFh+M3rh9hQ2eh0FBERV3FNUQPkeD00dPQ7HUNExFVcVtSxHOtUUYuIjOWyotaIWkTkg1xV1NleDy3dAwwO+5yOIiLiGq4q6hyvB2uhUdMfIiKjXFXU2d5YAM1Ti4iM4aqizvF6ADRPLSIyhquKOjtQ1Mc6+hxOIiLiHq4q6iRPNAnTojSiFhEZw1VFDf5R9TEVtYjIKNcVdY7Xw1EVtYjIKNcVdXaSR3PUIiJjuK6oc7wemroGGBrRphcREXBhUWd7Y7EWmrsGnI4iIuIKritqraUWEXk/9xV18om11CpqERFwY1En+beRN+gDRRERwIVFnRQbRWx0pKY+REQCXFfUxhhytOlFRGSU64oa/LsTNfUhIuLn2qLWiFpExM+VRZ3j9dDYNcCIzzodRUTEceMuamNMpDFmhzHm2WAGAv9Nbkd8lpZubXoRETmbEfVtQGWwgox1YtPL0XbNU4uIjKuojTH5wLXAfcGN4/e3GwhonlpEZLwj6juB7wGnvFKSMWa9MabcGFPe3Nx8XqFyvCc2vaioRUTOWNTGmOuAJmvtttO9z1p7r7W2zFpblpGRcV6hUuKiiYmK0E1uRUQY34h6NfB3xphq4FFgrTHmoWCGOrHpRSNqEZFxFLW19gfW2nxrbRFwE7DBWvv3wQ7m352oDxNFRFy5jhr889QaUYuIQNTZvNla+yrwalCSfEC210NjZz8+nyUiwkzGrxQRcSUXj6g9DI1YWnq06UVEwptrizo7SWupRUTAxUWttdQiIn6uLer8FH9RV7f0OJxERMRZri3qlPgY8pJjqajvcDqKiIijXFvUAIvyvSpqEQl7ri7qhfleao730tE75HQUERHHuLqoF+UlA2hULSJhzdVFvTDPC8Cu+naHk4iIOMfVRe2Ni2Z6WhwVdRpRi0j4cnVRAyzI0weKIhLeXF/Ui/K81LX10doz6HQUERFHuL6oF+b756k1qhaRcOX6ol4Q+ECxok4fKIpIeHJ9USd5opmRHs8ufaAoImHK9UUN/ukPTX2ISLiaGkWd56Who5+mLl1JT0TCz5Qo6kX5/h2KuzWqFpEwNCWKen5uEsZARV2n01FERCbdlCjq+GlRzMxIoEJbyUUkDE2Jogb/xhet/BCRcDRlinphvpemrgEaO/WBooiElylT1EsK/B8obj7c6nASEZHJNWWKelF+Milx0WysanI6iojIpJoyRR0ZYVgzO5NX9zYx4rNOxxERmTRTpqgBLp+TSVvvEDtrtfpDRMLHlCrqy0oziIwwmv4QkbAypYraGxfNBYUpbFBRi0gYmVJFDbB2bibvNXTS0NHndBQRkUkx9Yp6TiYAG6uaHU4iIjI5plxRl2YmkJccq+kPEQkbU66ojTGsnZPJWwda6B8acTqOiEjQTbmiBv/0R9/QiHYpikhYmJJFfdHMNDzREWyobHQ6iohI0J2xqI0xHmPMFmPMu8aYPcaYf5mMYKfjiY5k9cx0NuxtwlrtUhSR0DaeEfUAsNZauxhYAqwzxqwMbqwzu2JuFrWtfew5qpsJiEhoO2NRW7/uwNPowMPxYew1C7OJiYrgz+W1TkcREQmqcc1RG2MijTE7gSbgZWvt5pO8Z70xptwYU97cHPw1zslxMVw1P5undx7V6g8RCWnjKmpr7Yi1dgmQDyw3xiw4yXvutdaWWWvLMjIyJjrnSX2mrICOviFefk8fKopI6DqrVR/W2nbgVWBdUNKcpVUz08hLjuVPmv4QkRA2nlUfGcaY5MD3scBHgKpgBxuPiAjDDRfk8+aBFuraep2OIyISFOMZUecAG40xu4Ct+Oeonw1urPH7dFk+AI9vq3M4iYhIcESd6Q3W2l3A0knIck7yU+JYPTOdP5fX8e21pUREGKcjiYhMqCm5M/GDbrywgPr2Pt4+dNzpKCIiEy4kivqj87JI8kTx2FZ9qCgioSckitoTHcknlubx4u5jNHX2Ox1HRGRChURRA3z54mKGfT7uf/Ow01FERCZUyBT19LR4rluUy0Pv1NDRO+R0HBGRCRMyRQ3w9TUz6Rkc4fdvVzsdRURkwoRUUc/NSeKKOZn87q3D9A4OOx1HRGRChFRRA9x6+Uzaeod4ZItWgIhIaAi5or5geirLi1O5741DDA77nI4jInLeQq6oAb5xeQkNHf08vaPe6SgiIuctJIv60tJ0FuQl8YuNBxgY1rWqRWRqC8miNsbwvavmcKS1lz9sqnE6jojIeQnJoga4dFYGa2ZncPeG/RzvHnA6jojIOQvZogb44TVz6R0c4a5X9jsdRUTknIV0UZdmJfLZ5YU8vPkI+xu7nI4jInJOQrqoAW7/SClxMZH85PlKp6OIiJyTkC/qtIRpfGttCRv3NvP6vuDfHV1EZKKFfFED3LKqiOlpcfz4mT30D2m5nohMLWFR1NOiIvnJJxZyuKWHn7+8z+k4IiJnJSyKGmB1STo3Ly/gt28c4t3adqfjiIiMW9gUNcAPrplLZqKHf3r8Xe1YFJEpI6yKOskTzU8+uYB9jd38cuNBp+OIiIxLWBU1wNo5WXxiaR6/2niA9452Oh1HROSMwq6oAf7PdfNIjovhtkd36AYDIuJ6YVnUKfEx3PmZJRxo7ubHf9njdBwRkdMKy6IGuLg0nW9eXsKft9Xx5PY6p+OIiJxS2BY1wG1XlLK8OJUfPb2bA03dTscRETmpsC7qqMgI7r5pKZ7oSL75x+3atSgirhTWRQ2Q7fVwx42LqTrWxfef2IW11ulIIiLvE/ZFDbBmdib/dNVsnt55lF+9qvXVIuIuUU4HcItb18xkf2MX/++lvczMiGfdghynI4mIABpRjzLG8NNPLWJJQTLfeexddtd3OB1JRARQUb+PJzqSe79wASlx0fzDH8o51tHvdCQRERX1B2UmevjtLWV09g3x+fs309Yz6HQkEQlzZyxqY0yBMWajMabSGLPHGHPbZARz0vxcL7+9pYya1l6+9OBWega0zVxEnDOeEfUw8F1r7VxgJfANY8y84MZy3qqZ6fznzUvZVdfOPz60TZdFFRHHnLGorbUN1trtge+7gEogL9jB3OCq+dn8+6cW8cb+Fr7z2E6GR3xORxKRMHRWy/OMMUXAUmDzSX62HlgPUFhYOAHR3OHTZQV09A3xb89VEmF2cudnlhAVqal9EZk84y5qY0wC8ARwu7X2QxdyttbeC9wLUFZWFlLb+756yQxGfJb/+0IVPmu566alRKusRWSSjKuojTHR+Ev6YWvtk8GN5E5fu2wmkRGGf3uukhHfdv7z5mXERKmsRST4xrPqwwD3A5XW2juCH8m9vnrJDH58/Txe2tPIrQ9v00WcRGRSjGdIuBr4PLDWGLMz8LgmyLlc60uri/nXjy/glaomPn//Zjp6h5yOJCIh7oxTH9baNwEzCVmmjM+vnE5KXDTfeWwnN/7mbX7/5eVkez1OxxKREKVJ1nN03aJcHvzScurb+/jUrzdxoKnL6UgiEqJU1OdhdUk6j65fycDwCJ/81SbeOtDidCQRCUEq6vO0IM/LU7euJtvr4QsPbOHhzTVORxKREKOingAFqXE88fVVXFqazg+f2s0/P7NHuxhFZMKoqCdIoiea+265kK9cXMyDm6r54u+20qor74nIBFBRT6DICMP/vm4eP/vUIrZUt3Ld3W+ws7bd6VgiMsWpqIPgxgsLeOIfVxERYfj0PZt46J0a3TRXRM6ZijpIFuZ7efZbF7O6JJ0fPb2b2x7dSVe/NseIyNlTUQdRclwMD9xyId+9chbPVTRw7d1vaipERM6aijrIIiIM37qilMfWr2TEZ7nh15u457WD+HyaChGR8VFRT5KyolSe//YlfHR+Fj99oYqbf/sOta29TscSkSlART2JvHHR/PKzy/jZDYvYc7STdXe+zqNbjuiDRhE5LRX1JDPGcGNZAS/efgmL8pP5/pMVfPnBrTR09DkdTURcSkXtkPyUOB7+6gr++fp5vH3oOFfe8ToPvVOjuWsR+RAVtYMiIgxfXF3MS7dfyuICLz96ejc33fsOB5u7nY4mIi6ionaB6WnxPPSVFfzshkVUHevk6jvf4I7/2as7yIgIoKJ2jRNz13/97mVcvTCbuzcc4Mqfv8aGqkano4mIw1TULpOZ6OGum5byx6+uICYygi8/WM4//KGcmuM9TkcTEYeoqF1qVUk6L9x2Kd9bN5u3DrRw5R2v89MXqugeGHY6mohMMhW1i8VERXDrmhI2/q81XLc4h3teO8jl//Eqf9pay4hWh4iEDRX1FJCV5OGOG5fw1K2ryE+J5XtP7OKau95g494mbZYRCQMq6ilkaWEKT359Fb/87DL6h0f40u+28rn7NlNR1+F0NBEJIhX1FGOM4dpFObz8ncv48fXzqGzo5PpfvMnXH9rG/kbdCV0kFJlg/K9zWVmZLS8vn/C/Vz6sq3+I+944zP1vHqZ3cJiPL83j9itmUZgW53Q0ETkLxpht1tqyk/5MRR0aWnsGuee1g/x+UzXDPssnl+bxzbUlTE+LdzqaiIyDijqMNHX2c89rh3h4cw3DPsvHl+TxjctnMiMjweloInIaKuowNLawB0d8XLMwh2+sKWFebpLT0UTkJFTUYay5a4AH3jrMf71dQ/fAMGvnZPK1S2ewvDgVY4zT8UQkQEUtdPQO8Ye3q3ngrcO09Q6xuCCZr106g6vmZxMZocIWcZqKWkb1DY7w+LZa7nvzMDXHe5meFscXVxXx6bICEqZFOR1PJGypqOVDRnyWl/Yc4743DrH9SDuJnihuurCAL1xUREGqlvaJTDYVtZzWjiNtPPBWNc9XNGCt5Yq5WXxxVRGrZqZpHltkkqioZVyOtvfx8OYaHtlSS2vPICWZCXx+5XQ+sSyPJE+00/FEQtp5FbUx5gHgOqDJWrtgPL9QRT219Q+N8HxFA7/fVM27dR3ERkfysSW5fG7FdBbme52OJxKSzreoLwW6gT+oqMPPrrp2/rj5CH/ZeZS+oREW5CVx04WFfGxJLokaZYtMmPOe+jDGFAHPqqjDV2f/EE9tr+eRLUeoOtZFbHQk1y7K4TMXFlA2PUVz2SLn6XRFrfVYMi5JnmhuWVXEFy6azq66Dh7dWsszO+t5fFsdxenx3HBBPp9clkeON9bpqCIhZ8JG1MaY9cB6gMLCwgtqamomKKK4Vc/AMC/sPsafymvZcriVCAOrS9L51LJ8Pjo/i7gYjQNExktTHxJ0Ncd7eHxbHU/tqKeurY/4mEjWLcjh40tzWTUzXbsfRc5ARS2TxuezbK1u5cnt9Txf0UDXwDAZidO4flEuH1+ay8I8r+azRU7ifFd9PAKsAdKBRuDH1tr7T/dnVNQC/mV+G6qaeHpHPa/ubWZwxEdRWhzXL87l+sW5zMpKdDqiiGtow4s4rqN3iBf3NPDf7zaw6WALPguzsxK5ZmEO1y7KoSRT18uW8KaiFldp7hrg+YoGntvVwNaaVuyY0r56YTalmQmaHpGwo6IW12rs7OeFigaeq2igvKYNa2FGRjxXL8hm3fwcFuQlqbQlLKioZUpo6uznpfcaeXF3A+8camXEZ8n1evjo/Gw+Oi+L5cWpREVGOB1TJChU1DLltPUM8tfKRl7a08gb+5sZGPbhjY3m8tkZXDkvm0tnpWsLu4QUFbVMab2Dw7y+r5mX32tiQ1Ujbb1DREcaVs5IY+2cTD4yN0vX0JYpT0UtIWN4xMf2I+38tbKRVyobOdjcA0BpZgJr52SyZnYmZUUpRGuKRKYYFbWErOqWHjZUNfFKVSNbDrcyNGJJnBbFxaXprJmdwWWzMsn2epyOKXJGKmoJC90Dw7x1oIWNVU28ureZY539AMzJTuSyWRlcUppBWVEKnuhIh5OKfJiKWsKOtZZ9jd28ts9f2uXVbQyO+PBER7CiOI1LStO5pDSDWVlasy3uoKKWsNc7OMw7h47z+r4WXt/XzKEW/9x2ZuI0Li5J5+LSdFaXpJOVpGkScYauRy1hLy4mirVzslg7JwuA+vY+3tzfzBv7W3h1XzNP7qgHYGZGPKtL0lk1M52VM1JJjotxMrYIoBG1CD6fpfJYJ5sOHOetgy1sOdxK7+AIxsC8nCRWzUxj5Yw0LixO1U1+JWg09SFyFgaHfeyqa2fTweNsOtjC9pp2Bkd8RBhYkOdlRXEqK4r9xe2NVXHLxFBRi5yH/qERth9p451Drbxz6Dg7j/iL2xiYm53EihmprChOpawolfSEaU7HlSlKRS0ygfqHRthxpJ3Nh4+z+VArO2rb6B/yAf457uXFqZRNT+XColQKUmO1qkTGRUUtEkSDwz4q6jvYWt3KlsOtlFe30tk/DPhXlZQVpXDB9FTKpqcwLzdJuyblpFTUIpPI57Psb+pma3UrW6tbKa9uo769D4DY6EgWF3hZVpjCBdNTWFaYQkq8VpaIilrEccc6+imv8Zf2jiNt7DnaybDP/99ecXo8SwuTWVaYwtLCZGZnJepyrmFIRS3iMn2DI+yqa2fbkTZ2HGlnx5E2WroHAf+oe1G+lyWFySwtSGZJQYquVxIGtOFFxGViYyJZMSONFTPSAP+W97q2PrafKO7adh548zBDI/6BVFbSNJYUJLO4IJnF+ckszPdqTXcYUVGLuIAxhoLUOApS4/jYkjzAv7rkvYZO3q1t593adnbWtvPSnsbRPzMjI57F+cksyveyKN/LvBwvsTG64FQoUlGLuJQnOpJlhf4PHE9o7x1kV10Hu+ra2VnbwVsHWngqsP09MsJQmpnAwjwvC/O9LMzzMjcnSVcLDAGaoxaZ4ho7+0fLu6K+g4q6Do73+Oe7T5T3gjx/cS/IS2JuThJxMRqjuY0+TBQJI9Zajnb0U1HXzu76TirqO9hd/7fyNgZmpMczP9fL/Nyk0a9aJugsfZgoEkaMMeQlx5KXHMu6BTmAv7yPdfazp76T3Uc72HO0k/LqVp559+jon8v1epiXm8S8nKTAVy/5KbFERGhnpdNU1CJhwBhDjjeWHG8sH5mXNfp6a88glQ2d7AmUd2VDJxuqmggs8SZhWhRzshOZm5MUeCQyOztRUyeTTFMfIvI+/UMj7D3WRWWDv7jfa+ikqqGLrgH/tnhjYHpqHHOyk5iTk8ic7ETmZCdRmBqn0fd50NSHiIybJzrSv167IHn0tRPrvCsbOqkKlPjeY1289N4xToz1YqMjmZWVwOzsRGZnJzE7yz/6Tk+I0YWpzpNG1CJyzvoGR9jX2EXVMX+B7w08TnxwCZAaH8PsrERmZSUwKzuR2VmJlGYl6lreH6ARtYgERWzMh0ffAM1dA+xr9Je2v8i7eHxbHT2DI6PvyUqaxqysREoyE5gVKPKSTBX4yaioRWTCZSROIyNxGqtL0kdfO7FscN+xLvY2+gt8f2M3j26ppW/obwWemTiN0qwESjP9JV6amUBJZgJpYXxTBhW1iEyKscsGL5+TOfq6z2epb+9jf5O/uPc1dnOgqYs/l9e+bwSeEhdNSaC0Z2YkMDMzgZKMBPKSQ38JoYpaRBwVEfG365ycuEs8+EfgDR397Gvs4kBTNwebuznQ1M2Lu4/R1js0+j5PdATF6QnMzIhnZkYCM8Z8DZVlhKFxFCIScowx5CbHkpscy5rZme/7WWvP4GhxH2jq5lBzN7vqOniuooGx6yNyvR5mBEp7Rnr86Pe53qk1Ch9XURtj1gF3AZHAfdbanwY1lYjIaaTGx5Aa778v5Vj9QyNUH+/hUHMPB5u6OdTSw8Hmbp7aXj+6DhxgWlQERWnxFKfHU5zh/zoj3f81Nd59ywnPWNTGmEjgl8CVQB2w1RjzjLX2vWCHExE5G57oSP9GnOyk971uraW5e4BDzT0cbvE/DjX3sK+pi79WNo7ebQcg0RNFcXo8RWnxFKXHU5weN1rqyXHOXA9lPCPq5cABa+0hAGPMo8DHABW1iEwJxhgyEz1kJnpYGbhZwwnDIz7q2vr85d3SQ3VLD9XHe9h+pI3/3nX0fVMpyXHR/gJPi2N6oLynp/mLPDkuOmgj8fEUdR5QO+Z5HbDig28yxqwH1gMUFhZOSDgRkWCLioygKN0/er78Az8bGB6htrWXwy29VLf0cPh4DzXHe9ha3cZf3n1/iSd5opidncifvnbRhBf2eIr6ZL/xQ9sZrbX3AveCf2fieeYSEXHctKhISjITKclM/NDPTpR4zfFeqo/3UnO8h8FhX1BG1eMp6jqgYMzzfODoKd4rIhIWTlfiE20896TfCpQaY4qNMTHATcAzwY0lIiInnHFEba0dNsZ8E3gJ//K8B6y1e4KeTEREgHGuo7bWPg88H+QsIiJyEuOZ+hAREQepqEVEXE5FLSLicipqERGXU1GLiLhcUO6ZaIxpBmrO8Y+nAy0TGGcq0DGHvnA7XtAxn63p1tqMk/0gKEV9Powx5ae6wWOo0jGHvnA7XtAxTyRNfYiIuJyKWkTE5dxY1Pc6HcABOubQF27HCzrmCeO6OWoREXk/N46oRURkDBW1iIjLuaaojTHrjDF7jTEHjDHfdzpPMBhjCowxG40xlcaYPcaY2wKvpxpjXjbG7A98TXE660QzxkQaY3YYY54NPC82xmwOHPNjgWudhwxjTLIx5nFjTFXgfF8U6ufZGPOdwL/r3caYR4wxnlA7z8aYB4wxTcaY3WNeO+l5NX53BzptlzFm2bn+XlcU9Zg7nV8NzANuNsbMczZVUAwD37XWzgVWAt8IHOf3gVestaXAK4HnoeY2oHLM838Hfh445jbgK46kCp67gBettXOAxfiPPWTPszEmD/g2UGatXYD/2vU3EXrn+UFg3QdeO9V5vRooDTzWA78+599qrXX8AVwEvDTm+Q+AHzidaxKO+y/AlcBeICfwWg6w1+lsE3yc+YF/wGuBZ/Hfh7MFiDrZ+Z/qDyAJOEzgw/oxr4fseeZvN8FOxX+d+2eBq0LxPANFwO4znVfgN8DNJ3vf2T5cMaLm5Hc6z3Moy6QwxhQBS4HNQJa1tgEg8DXTuWRBcSfwPcAXeJ4GtFtrhwPPQ+18zwCagd8FpnvuM8bEE8Ln2VpbD/wHcARoADqAbYT2eT7hVOd1wnrNLUU9rjudhwpjTALwBHC7tbbT6TzBZIy5Dmiy1m4b+/JJ3hpK5zsKWAb82lq7FOghhKY5TiYwL/sxoBjIBeLx/6//B4XSeT6TCft37paiDps7nRtjovGX9MPW2icDLzcaY3ICP88BmpzKFwSrgb8zxlQDj+Kf/rgTSDbGnLgVXKid7zqgzlq7OfD8cfzFHcrn+SPAYWtts7V2CHgSWEVon+cTTnVeJ6zX3FLUYXGnc2OMAe4HKq21d4z50TPALYHvb8E/dx0SrLU/sNbmW2uL8J/XDdbazwEbgRsCbwu1Yz4G1BpjZgdeugJ4jxA+z/inPFYaY+IC/85PHHPInucxTnVenwG+EFj9sRLoODFFctacnpgfM9F+DbAPOAj80Ok8QTrGi/H/r88uYGfgcQ3+OdtXgP2Br6lOZw3S8a8Bng18PwPYAhwA/gxMczrfBB/rEqA8cK6fBlJC/TwD/wJUAbuB/wKmhdp5Bh7BPwc/hH/E/JVTnVf8Ux+/DHRaBf4VMef0e7WFXETE5dwy9SEiIqegohYRcTkVtYiIy6moRURcTkUtIuJyKmoREZdTUYuIuNz/B9efU/CXU7+2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0, 100, num = 80)\n",
    "sns.lineplot(x = x, y = np.log(100 / (x + 1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFs give us a measure of the relevancy of a term to a document, but of course TFs for common terms will be inflated. So we can use IDFs to \"normalize\" the TFs and dampen the relevancy of terms that appear frequently across documents by virtue of being common terms. In other words, IDFs can serve as a factor to emphasize rare terms. This results in **term frequency inverse document frequency (TF-IDF) maxtrix**. \n",
    "\n",
    "$$\\text{TF-IDF} = \\text{TF(term, doc)} \\cdot \\text{IDF(term)}$$\n",
    "\n",
    "Applications of TF-IDF include\n",
    "\n",
    "- Characterize writing styles\n",
    "- Comparing authors\n",
    "- Determining original authors\n",
    "- Finding plagiarism\n",
    "\n",
    "The code in the cell below computes both simple TF and the cumulative of the term frequencies, starting from the most frequent terms to the least. We can use a built-in TF-IDF transformation function to do the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 6228, stop_words = 'english')\n",
    "clean_texts = tweet_df['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tf_idf_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'csr' (compressed sparse row) format is used by `sklearn` to store the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and sentiment analysis\n",
    "\n",
    "Now that we have a prepared TDM of the 160,000 tweets, let's build and evaluate models to classify the sentiment of these tweets. An outline of our process is as follows:\n",
    "\n",
    "- Use TDM or TF-IDF weighted TDM as features for training the model.\n",
    "- Use marked cases for training and evaluation of model.\n",
    "- Select a method for sparse matrix requires regularization from the following:\n",
    "  - Feature selection, is impractical since there are over one million features.\n",
    "  - SVD/PCA could be used to reduce dimensionality of the problem.\n",
    "  - In this case we will use the ridge and lasso methods offered in the  elasticnet model.\n",
    "\n",
    "Let's split the tf-idf sparse matrix. For training we will use 120,000 tweets to predict the 0,1 sentiment. The remaining 40,000 cases will be used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targets = np.array([y[0] for y in tweet_data])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 40000, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a logistic classifier on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs = lr.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7793333333333333\n",
      "Test accuracy: 0.75625\n"
     ]
    }
   ],
   "source": [
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the precision, recall and Fscore of the model for positive and negative tweets.\n",
    "\n",
    "Recall that a positive prediction here means a positive review, and so **precision** is the proportion of correct predictions among all positive predictions and **recall** is the proportion of correct predictions among all true positives. **F1** is the harmonic average of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14624  5432]\n",
      " [ 4318 15626]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.77204097 0.74204578]\n",
      "Recall   : [0.72915836 0.78349378]\n",
      "F1       : [0.74998718 0.76220672]\n",
      "Support  : [20056 19944]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Topic models\n",
    "\n",
    "It is often useful to allocate documents to one or more topics. This process can be useful in, say, information retrieval and search. Models to perform this allocation to topics are known as **topic models**. \n",
    "\n",
    "A powerful topic model is know as **latent Dirichlet allocation** or **LDA**. LDA is an unsupervised Bayesian learning model.  We can summarize the LDA model as follows:\n",
    "\n",
    "- The LDA model uses a fixed number of topics, $k$.\n",
    "- The model computes the posterior probability of a document containing a topic.\n",
    "- All other variables are estimated or **latent**, including the topics of each document.\n",
    "\n",
    "How does latent Dirichlet allocation work? It's a Bayesian model, so we need to define a likelihood and choose a prior. What we actually know are the term frequencies. What we want to know (the latent variable) is the topic distribution (allocation) for a given document. Based on the distribution, we can categorize the document.\n",
    "\n",
    "Since we don't know the allocation of topics in advance we generally use uniform priors across topics. The likelihood is taken from the term-document matrix. We leave it to the reader to learn more about the math behind it. Instead, let's try an example. In this example we apply LDA to a corpus of 20 business news articles from Reuters news wire concerning the oil industry. We will apply an LDA model with 5 topics ($k = 5$).\n",
    "\n",
    "As a first step we will load the corpus of these documents. Execute the code in the cell below to load the corpus and examine the contents of the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data set. Please wait.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 3000\n",
    "n_features = 500\n",
    "n_components = 10\n",
    "n_top_words = 5\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #{}: \".format(topic_idx)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"Loading data set. Please wait.\")\n",
    "news_data = fetch_20newsgroups(shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "small_data = news_data.data[:n_samples]\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the term document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_df = 0.9, min_df = 5, max_features = n_features, stop_words = 'english')\n",
    "tf = count_vectorizer.fit_transform(small_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train an LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=50.0,\n",
       "                          max_doc_update_iter=100, max_iter=5,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components, max_iter=5, learning_method='online', learning_offset=50.)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top words for each topic that was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: edu key com mail new\n",
      "Topic #1: god jesus believe does bible\n",
      "Topic #2: 17 year 11 12 10\n",
      "Topic #3: windows dos game team games\n",
      "Topic #4: don just people think like\n",
      "Topic #5: file program use files available\n",
      "Topic #6: people government jews turkish armenian\n",
      "Topic #7: drive card scsi disk pc\n",
      "Topic #8: space 00 50 nasa new\n",
      "Topic #9: ax max b8f 145 pl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = count_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Summary\n",
    "\n",
    "NLP applications extend far and wide, so we only stratched the surface here. Many of the modern breakthroughs in deep learning for example have been in NLP. One reason for this is that language data is abundant and the lack of structure in the data presents us with many challenges and learning opportunities. We hope this notebook exposed you to just some examples."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
