{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression, part 2\n",
    "\n",
    "In this notebook we expand on the ideas of linear regression as we try to resolve problems we encounter when we try to fit a regression model. More specifically, we talk about how to include categorical data in our model, how to deal with high multi-collinearity between features, and finally what to do if the response variable is binary. Although these topics are not necessarily related to each other, they are all good examples of how we can extend the ideas behind linear regression to address its shortcomings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [10, 7]\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as statsmodels\n",
    "import statsmodels.formula.api as sm\n",
    "import patsy as p\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "\n",
    "It's time to talk about what to do with categorical features. Recall that after expressing the math of linear regression using linear algebra, $\\pmb{X}$ ends up being a matrix with a column of 1s as its first column to account for the intercept. So we need to encode our categorical features as numeric columns that we can include in $\\pmb{X}$. It turns out that we have more than one option, although most of the time we get go with the default option. Let's read in our data and learn by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb2 = pd.read_csv('data/hsb2-2.csv')\n",
    "hsb2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Notice that we have a few categorical features we can use in the data, however in this case the categorical features are already encoded as numbers. For example `schtype == 1` means \"public\" and `schtype == 0` means \"private\". So what's stopping us from using these as is?\n",
    "\n",
    "- Using `sm.ols` train a model to predict writing score using reading score and socio-economic status (column named `ses`). Then examine the results by calling the fitted model's `summary2()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = 'write ~ read + ses', data = hsb2)\n",
    "ols_model = ols_model.fit()\n",
    "ols_model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is the coefficient for `ses` significant? Write your best interpretation of it based on what we learned so far. Do you see any problems with the interpretation?\n",
    "\n",
    "To encode a variable as categorical we wrap its name with a `C` function in the formula, e.g. `y ~ x + C(v)` if `v` is categorical. \n",
    "\n",
    "- Fit a new model this time encoding `ses` as categorical. What changes in the results table? Write the prediction equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = 'write ~ read + C(ses)', data = hsb2)\n",
    "ols_model = ols_model.fit()\n",
    "ols_model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we encode a variable as categorical, by default it creates a separate coefficient for all its categories except the first one. The first category becomes what we call **the baseline**, and the coefficients for each of the other categories capture the average \"effect\" of moving from the baseline to that category. So in the results above we can see that the coefficient for `C(ses)[T.2]` is -0.4746. This means that on average students with `ses == 2` have a score of 0.4746 less than the `ses == 1` (baseline) students. For an **orginal** categorical column like `ses`, both ways of fitting the model are valid, although the first way is more constrained. For **nominal** categorical columns like `race` or `gender`, only the second way is valid.\n",
    "\n",
    "So let's recode the `ses` column using proper names: low, middle and high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb2['ses'] = hsb2['ses'].replace({1: 'low', 2: 'middle', 3: 'high'})\n",
    "hsb2['ses'].value_counts(sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now the first category is `high`, whereas before it was `1` (which corresponds to `low`). This is because when we alphabetically sort the catogories by their new names, `high` appears first, whereas before `1` appeared first.\n",
    "\n",
    "- Refit the same model now that `ses` has been recoded. What differences do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = 'write ~ read + C(ses)', data = hsb2)\n",
    "ols_model = ols_model.fit()\n",
    "ols_model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foretunately, there's an easy way to choose which category to use as the **baseline**.\n",
    "\n",
    "- Refit the same model, but use this as `formula = \"write ~ read + C(ses, Treatment(reference = 'low'))\"`. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = \"write ~ read + C(ses, Treatment(reference = 'low'))\", data = hsb2)\n",
    "ols_model = ols_model.fit()\n",
    "ols_model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of coding shown above is called **dummy coding** (because it's what we get if we encode each category except the baseline using **dummy variables**). This is the most common type of encoding and the default one used by most statistical packages for regression models (linear regression, ANOVA, logistic regression, etc.). In machine learning, **dummy coding** is more commonly known as **one-hot encoding**, however in the latter case we usually do not drop any of the columns. Not dropping the baseline column can cause issues with convergence of the regression model, but this is only caused if there's no regularization, so in practice one-hot encoding the feature is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hsb2.loc[~hsb2.duplicated(subset = 'ses'), :].sort_values(['ses'])\n",
    "p.dmatrix(\"C(ses, Treatment('low'))\", data, return_type = \"dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other coding schemes that we can use (more on this topic [here](https://patsy.readthedocs.io/en/latest/API-reference.html#patsy.Treatment). Here's another one called deviation coding for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.dmatrix(\"C(ses, Sum)\", data, return_type = \"dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rerun the same model, but this time use deviation coding instead of dummy coding. What changes in the model summary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = \"write ~ read + C(ses, Sum)\", data = hsb2)\n",
    "ols_model = ols_model.fit()\n",
    "ols_model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a scatter plot of reading score (x-axis) vs writing score (y-axis). Then add a line plot of the prediction line for writing score (you will need to use your model to get predictions and add them as a new column to the data). Color-code the line plot by `hue = 'ses'` se we get a separate line plot for each value of `ses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb2['write_pred'] = ols_model.predict(hsb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'read', y = 'write', data = hsb2)\n",
    "sns.lineplot(x = 'read', y = 'write_pred', hue = 'ses', data = hsb2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit a new model now that also includes the interaction between `read` and `C(ses)`. Examine the model summary and report what changed. Then recreate the scatter plot with the prediction lines above. What looks different now? Can you interpret each of the coefficients based on the line plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = \"write ~ read * C(ses, Sum) + read:C(ses)\", data = hsb2)\n",
    "ols_model = ols_model.fit()\n",
    "hsb2['write_pred_w_interactions'] = ols_model.predict(hsb2)\n",
    "ols_model.summary2()\n",
    "\n",
    "sns.scatterplot(x = 'read', y = 'write', data = hsb2)\n",
    "sns.lineplot(x = 'read', y = 'write_pred_w_interactions', hue = 'ses', data = hsb2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! This was a long exercise but the goal was to slowly build our way up. There are other things we can test. For example, how does changing the encoding change the prediction? The answer is that it doesn't. Changing the encoding changes the coefficients and their interpretation, but those new coefficients still return the same prediction. What about including more than one categorical feature? What about the interaction of two categorical features? We leave it to you to try this and based on what you learned in this exercise you should be able to make a good guess as to what to expect.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in the example above, when we have categorical features in a model, we have some flexibility in how we want them encoded, but the general idea is to have a separate coefficient for each **level** (grouping) of a categorical feature, and another coefficent for each combination of the interactions of two categorical features (assuming we fit a model with interactions). Different choices for the **encoding** of the categorical features leads to different interpretations of their coefficients, but ultimately all lead to the same prediction. You can imagine that having **high-cardinality** categorical features can cause the model to quickly grow in complexity (number of coefficients), even when no interactions are being considered. In such cases, one option is to use **feature hashing**, which allows us to trade-off the size of the encoding with the risk of collision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component regression\n",
    "\n",
    "Imagine we are performing linear regression on $p$ features, where our model is as follows:\n",
    "\n",
    "$$Y = \\beta_{0} + \\beta_1X_1 + ... + \\beta_pX_p + \\text{error}$$\n",
    "\n",
    "So far for the linear models we fit, we took it for granted that the features are given to us, although we saw how we can apply transformations to one feature at a time, such as replacing $X$ with $\\log(X)$ or $X^2$. We also learned that having a lot of correlated features can introduce the problem of **multi-collinearity** which can make the predictions unstable (high variance). So let's now consider a transformation that will help in such a situation. Unlike previous transformation that only applied to one feature at a time, this new transformation takes as input **all** $p$ features and returns as its output a set of $p$ new features, although we usually only take the top $k$ where $k < p$. For this reason, PCA is an example of **dimensionality reduction**.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y &= \\beta_{0} + \\beta_1f_1(X_1, \\cdots, X_p) + \\cdots + \\beta_pf_p(X_1, \\cdots, X_p) + \\text{error} \\\\\n",
    "  &\\approx \\beta_{0} + \\beta_1f_1(X_1, \\cdots, X_p) + \\cdots + \\beta_kf_k(X_1, \\cdots, X_p) + \\text{error}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unlike the old features, the new features $f_1, \\cdots, f_p$ are completely un-correlated to each other by design, as we will see, so we can fit a linear regression model without worrying about **multi-collinearity**. What does $f_i(X_1, \\cdots, X_p)$ look like? The answer is that each $f_i()$ is a **linear combination** of the original features $X_1, \\cdots, X_p$:\n",
    "\n",
    "$$f_i = b_{i,0} + b_{i,1} X_1 + \\cdots + b_{i,p} X_p$$\n",
    "\n",
    "The question still remains, how do we find such magical $f_i$ functions that assures us that the new features, $f_i$ taken together are 100% independent of each other? Notice that we are referring to $f_i$ both as a function and as a feature. It would be more correct to say call the output of $f_i$ a feature, but from context it is usually clear if we mean $f_i$ the function or $f_i$ the feature.\n",
    "\n",
    "The method of finding these very important components is called **principal component analysis** or PCA. PCA is an **unsupervised learning** algorithm that takes $X_1, \\cdots, X_p$ as inputs and returns $f_1, \\cdots, f_p$ as output. We call $f_1, \\cdots, f_p$ the **principal components**. We need to normalize the data prior to performing PCA.\n",
    "\n",
    "After performing PCA, we can then use the top $k$ principal components as features for a linear regression model with $Y$ as target. In other words, we are using PCA (unsupervised learning) to create features that we then use to train a linear regression algorthim (supervised learning). This is why we refer to this technique as **principal component regression**.\n",
    "\n",
    "Time to see this in action. Here's a scatter plot of two normalized features `X_1` and `X_2`. Note that we only have features, no target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "X = np.dot(np.random.randn(100, 2), np.random.rand(2, 2))\n",
    "X = StandardScaler().fit_transform(X) # normalize the data\n",
    "\n",
    "sns.scatterplot(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the two principal components that result from running PCA on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X)\n",
    "pca_df = pd.DataFrame(data = pca.transform(X), columns = ['pc1', 'pc2'])\n",
    "\n",
    "print(pca_df.shape)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And are the principal components plotted against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot('pc1', 'pc2', data = pca_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we interpret the principal components: \n",
    "\n",
    "1. The first principal component is a linear combination of $X_1$ and $X_2$ in the direction where the data has the most variation. You can see that in the above scatter plot by how the first principal component has a wider range of values than the second one.\n",
    "1. The second principal component is another linear combination of $X_1$ and $X_2$ in the direction that has the most variation while being perpendicular to the first one. We can see that in the above scatter plot by how the second principal component looks uncorrelated to the first one.\n",
    "\n",
    "This argument can be extended to higher dimensions. So in $p$ dimensions we have $p$ principal components.\n",
    "\n",
    "Here are the two principal components and the amount of variation they each capture. Note that in this context, variation is like information. The more variation we capture, the more information we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_var = pca.explained_variance_\n",
    "components = pca.components_\n",
    "\n",
    "print(\"Variance explained by each PC: {}\".format(exp_var))\n",
    "print(\"Principal components: \\n{}\".format(components))\n",
    "\n",
    "v1 = 2 * components[0] * np.sqrt(exp_var[0])\n",
    "v2 = 2 * components[1] * np.sqrt(exp_var[1])\n",
    "c = (0, 0) # center is at 0, 0 because of our standardization\n",
    "\n",
    "sns.scatterplot(X[:, 0], X[:, 1], alpha = 0.5)\n",
    "plt.annotate('', c + v1, c, arrowprops = {'arrowstyle': '->', 'shrinkA': 0, 'shrinkB': 0, 'linewidth': 3})\n",
    "plt.annotate('', c + v2, c, arrowprops = {'arrowstyle': '->', 'shrinkA': 0, 'shrinkB': 0, 'linewidth': 3})\n",
    "plt.axis('equal')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the above variances and components? We can think of the principal components as performing a rotation of the axes, and the variance explained as how much we stretch each axis so that we express the original data in terms of the new axes.\n",
    "\n",
    "Once we have the principal components, we can then transform each data point by performing a simple matrix multiplication. The result is the same as what we obtained by calling `pca.transform` earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(X, pca.components_)[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra terms, we describe this as a **change of bases**: Our data expresses each point in terms of the coordinates $X_1$ and $X_2$. While those coordinates are perpendicular to each other, the data we have suggests that they share a lot of variation. PCA changes our coordinates to use the axes spanned by the principal components, and expresses the data points in terms of these new bases. The new coordinates align themselves with the data more naturally. This is why PCA is related to the idea **singular value decomposition (SVD)** in linear algebra, but we leave it to the reader to learn this in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's try our hand at **principal compenent regression**. This means we are first going to use **principal component analysis** to create a new set of features and then use those features to train a linear regression model.\n",
    "\n",
    "We are going to use the HSB2 data set from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb2 = pd.read_csv('data/hsb2-2.csv')\n",
    "hsb2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsb2['ses'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a linear regression model to predict a student's math score using reading, writing and science scores. Decide if you should train the model with main effects only, or include two-way interactions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_model = sm.ols(formula = \"math ~ read * write * science\", data = hsb2) <- the interaction terms here lead to SUPER HIGH P values... \n",
    "ols_model = sm.ols(formula = \"math ~ read + write + science\", data = hsb2)\n",
    "ols_model = ols_model.fit()\n",
    "hsb2['math_pred'] = ols_model.predict(hsb2)\n",
    "ols_model.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'read', y = 'math', data = hsb2)\n",
    "sns.lineplot(x = 'read', y = 'math_pred', hue = 'ses', data = hsb2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examine the data to see if you should be worried about multi-collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a pairs plot to see if there's a relationship between reading / writing / science\n",
    "pair_cols = ['read', 'write', 'science']\n",
    "sns.pairplot(hsb2[pair_cols]);\n",
    "\n",
    "# yep... multi-collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply PCA to the features and extract the principal components. How correlated are the principal component with the original features? You can start with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_orig = ['read', 'write', 'science']\n",
    "features_pca = ['pc_' + str(i+1) for i in range(len(features_orig))]\n",
    "pca = PCA(n_components = 3)\n",
    "\n",
    "pca.fit(hsb2[features_orig])\n",
    "pca_features = pd.DataFrame(data = pca.transform(hsb2[features_orig]), columns=features_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate([hsb2[features_orig], pca_features], axis = 1)\n",
    "hsb2_new = pd.DataFrame(features, columns=features_orig + features_pca)\n",
    "hsb2_new['math'] = hsb2['math']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform PCA and get the components\n",
    "features = np.concatenate([hsb2[features_orig], pca_features], axis = 1)\n",
    "hsb2_new = pd.DataFrame(features, columns = features_orig + features_pca)\n",
    "hsb2_new['math'] = hsb2['math']\n",
    "\n",
    "## examine the correlation between original features and PCs\n",
    "## HINT: look at a heatmap of the correlation matrix, and let cmap = 'RdBu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Did you remember to normalize the data first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a linear regression model to predict math score, but this time use the principal components as features. How many principal components would you include? Compare the model performance to the previous one. It may help to look at the following bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(hsb2_new.corr(), cmap = 'RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(features_pca, pca.explained_variance_, color = 'grey');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula= \"math ~ pc_1\", data = hsb2_new).fit()\n",
    "print(ols_model.summary2())\n",
    "\n",
    "# the R squared of the PCA regression is the same a the traditional regression...\n",
    "# this suggests that we're not losing any predictive ability by using this dimensional reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpret your model's coefficents. Would you prefer the model with PCA or without?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can't really intrepret this coefficient, as it's a dimensional reduction of the other 3 terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, interpreting coefficients becomes much more challenging once we perform feature engineering steps like PCA was here. This trade-off between interpretability and model accuracy is another manifestation of the **bias variance trade-off**.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Until now, we have been working strictly with linear **regression models**, which assumes that we have a numeric target. With a slight modification of linear regression, we can also predict binary targets (`{False, True}` or `{0, 1}` etc.). The result is called **logistic regression**. As mentioned earlier, the word \"regression\" in logistic regression refers to its relationship to linear regression, but logistic regression is a classification algorithm.\n",
    "\n",
    "The response of the linear model is transformed to the log likelihood using a sigmoidal function, also known as the **logistic map** or the **sogmoid function**:\n",
    "\n",
    "$$f(x | x_0, \\kappa) = \\frac{1}{1 + e^{-\\kappa(x - x_0)}}$$\n",
    "\n",
    "where $\\kappa$ is a measure of the steepness of the curve, as you can see here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the logistic transformation function (f(x) above)\n",
    "x_seq = np.linspace(-7, 7, 100)\n",
    "\n",
    "def log_fun(x, center = 0, scale = 1):\n",
    "    e = np.exp(-scale*(x - center))\n",
    "    log_out = 1./(1. + e)\n",
    "    return log_out\n",
    "\n",
    "log_fun_vectorized = np.vectorize(log_fun)\n",
    "\n",
    "for ii in range(10):\n",
    "    log_y = log_fun_vectorized(x_seq, scale = ii)\n",
    "    sns.lineplot(x_seq, log_y, alpha = 1.0/(ii + 1), color = 'blue');\n",
    "    \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('logistic(x)');\n",
    "plt.legend(['kappa = ' + str(ii) for ii in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this a bit more concrete with a simple example. Say we have a linear model:\n",
    "\n",
    "$$\\hat{Y} = b_0 + b_1 X$$\n",
    "\n",
    "Now, depending on the value of $\\hat{Y}$ we want to classify the output from a logistic regression model as either `0` or `1`. We can use the linear model in the logistic function as follows:\n",
    "\n",
    "$$f(\\hat Y|X) = f(b_0 + b_1 X) = \\frac{1}{1 + e^{-\\kappa(b_0 + b_1 X)}}$$\n",
    "\n",
    "We can ignore $\\kappa$ since it can be factored into the coefficients. In this way we transform the continuous output of the linear model defined on $-\\infty \\le \\hat Y \\le \\infty$ to a binary response, $0 < f(\\hat Y|X) < 1$. If we do the transformation in reverse we get\n",
    "\n",
    "$$e^{-(b_0 + b_1 X)} = \\frac{1}{f(\\hat Y|X)} - 1 = \\frac{1 - f(\\hat Y|X)}{f(\\hat Y|X)} \\quad\\Rightarrow\\quad b_0 + b_1 X = \\log(\\frac{f(\\hat Y|X)}{1 - f(\\hat Y|X)})$$\n",
    "\n",
    "The right-hand side shows the relationship between linear and logistic regression: Logistic regression is what we get if we have a binary response variable and run linear regression, not to predict $Y$ directly, but to predict a transformation of $Y$.\n",
    "\n",
    "What about interpreting our results? First note that because of the nature of the logistic map, we can think of $f(\\hat Y|X)$ as a probability: $f(\\hat Y|X) = P(Y = 1|X)$. This means we can rewrite the right-hand side of the above equation as \n",
    "\n",
    "$$b_0 + b_1 X = \\log(\\frac{f(\\hat Y|X)}{1 - f(\\hat Y|X)}) = \\log(\\frac{P(Y = 1|X)}{P(Y = 0|X)})$$\n",
    "\n",
    "We call the ratio $P(Y = 1) / P(Y = 0)$ the **odds ratio**, and so the quantity on the right-hand side of the above equation is called the **log odds-ratio**.\n",
    "\n",
    "To summarize, when we have a binary response variable $Y \\in \\{0, 1\\}$, logistic regression fits a linear regression model to predict the log odds-ratio of $Y = 1$ using our features. Once we have the prediction, we can express it as a probability $P(Y = 1 | X)$ by rewriting it:\n",
    "\n",
    "$$P(Y = 1|X) = \\frac{1}{1 + e^{-(b_0 + b_1 X)}}$$\n",
    "\n",
    "We can easily extend this idea to many features and nothing really changes other than having additional terms in the exponential term.\n",
    "\n",
    "Unlike linear regression, we do NOT have a **closed-form solution** in logistic regression. So we find the coefficients $b_i$ using optimization. The function we optimize is the likelihood given by \n",
    "\n",
    "$$L(\\pmb{\\beta}) = \\prod_j P(Y_j = 1|X_j)^{Y_j} \\times (1 - P(Y_j = 1|X_j))^{1 - Y_j}$$\n",
    "\n",
    "although computationally it is easier to optimize the log of the likelihood, conviniently called the **log-likelihood**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Let's re-load the HSB2 data and use it to illustrate logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>female</th>\n",
       "      <th>race</th>\n",
       "      <th>ses</th>\n",
       "      <th>schtyp</th>\n",
       "      <th>prog</th>\n",
       "      <th>read</th>\n",
       "      <th>write</th>\n",
       "      <th>math</th>\n",
       "      <th>science</th>\n",
       "      <th>socst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>53</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  female  race  ses  schtyp  prog  read  write  math  science  socst\n",
       "0   70       0     4    1       1     1    57     52    41       47     57\n",
       "1  121       1     4    2       1     3    68     59    53       63     61\n",
       "2   86       0     4    3       1     1    44     33    54       58     31\n",
       "3  141       0     4    3       1     3    63     44    47       53     56\n",
       "4  172       0     4    2       1     2    47     52    57       53     61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsb2 = pd.read_csv('data/hsb2-2.csv')\n",
    "hsb2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a new column called `math_high` that is equal to 1 when a student's math score is above the 80% percentile, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>female</th>\n",
       "      <th>race</th>\n",
       "      <th>ses</th>\n",
       "      <th>schtyp</th>\n",
       "      <th>prog</th>\n",
       "      <th>read</th>\n",
       "      <th>write</th>\n",
       "      <th>math</th>\n",
       "      <th>science</th>\n",
       "      <th>socst</th>\n",
       "      <th>math_ntile</th>\n",
       "      <th>math_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>53</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>59</td>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>58</td>\n",
       "      <td>51</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>55</td>\n",
       "      <td>52</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>51</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>65</td>\n",
       "      <td>51</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "      <td>60</td>\n",
       "      <td>71</td>\n",
       "      <td>61</td>\n",
       "      <td>71</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>63</td>\n",
       "      <td>57</td>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "      <td>56</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>56</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>56</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>60</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>55</td>\n",
       "      <td>61</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>39</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  female  race  ses  schtyp  prog  read  write  math  science  socst  \\\n",
       "0    70       0     4    1       1     1    57     52    41       47     57   \n",
       "1   121       1     4    2       1     3    68     59    53       63     61   \n",
       "2    86       0     4    3       1     1    44     33    54       58     31   \n",
       "3   141       0     4    3       1     3    63     44    47       53     56   \n",
       "4   172       0     4    2       1     2    47     52    57       53     61   \n",
       "5   113       0     4    2       1     2    44     52    51       63     61   \n",
       "6    50       0     3    2       1     1    50     59    42       53     61   \n",
       "7    11       0     1    2       1     2    34     46    45       39     36   \n",
       "8    84       0     4    2       1     1    63     57    54       58     51   \n",
       "9    48       0     3    2       1     2    57     55    52       50     51   \n",
       "10   75       0     4    2       1     3    60     46    51       53     61   \n",
       "11   60       0     4    2       1     2    57     65    51       63     61   \n",
       "12   95       0     4    3       1     2    73     60    71       61     71   \n",
       "13  104       0     4    3       1     2    54     63    57       55     46   \n",
       "14   38       0     3    1       1     2    45     57    50       31     56   \n",
       "15  115       0     4    1       1     1    42     49    43       50     56   \n",
       "16   76       0     4    3       1     2    47     52    51       50     56   \n",
       "17  195       0     4    2       2     1    57     57    60       58     56   \n",
       "18  114       0     4    3       1     2    68     65    62       55     61   \n",
       "19   85       0     4    2       1     1    55     39    57       53     46   \n",
       "\n",
       "    math_ntile  math_high  \n",
       "0     0.546667          0  \n",
       "1     0.706667          0  \n",
       "2     0.720000          0  \n",
       "3     0.626667          0  \n",
       "4     0.760000          0  \n",
       "5     0.680000          0  \n",
       "6     0.560000          0  \n",
       "7     0.600000          0  \n",
       "8     0.720000          0  \n",
       "9     0.693333          0  \n",
       "10    0.680000          0  \n",
       "11    0.680000          0  \n",
       "12    0.946667          1  \n",
       "13    0.760000          0  \n",
       "14    0.666667          0  \n",
       "15    0.573333          0  \n",
       "16    0.680000          0  \n",
       "17    0.800000          1  \n",
       "18    0.826667          1  \n",
       "19    0.760000          0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsb2['math_ntile'] = hsb2['math'] / hsb2['math'].max()\n",
    "hsb2['math_high'] = hsb2['math_ntile'].apply(lambda x: 1 if x >= .80 else 0)\n",
    "hsb2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can a student's science score and the school type they attended be good predictors of a student's probability of being in the top 20% in math? For school type we use the column `schtyp`, where `1` is public and `2` is private school. HINT: Use `sm.logit` to train a logistic regression. You can pass it a formula just like the one in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.424661\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.237</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>math_high</td>          <td>AIC:</td>         <td>175.8643</td> \n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2020-08-13 21:05</td>       <td>BIC:</td>         <td>185.7592</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>200</td>        <td>Log-Likelihood:</td>    <td>-84.932</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>2</td>            <td>LL-Null:</td>        <td>-111.36</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>197</td>         <td>LLR p-value:</td>    <td>3.3473e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>7.0000</td>              <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>         <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th>  <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>-9.8102</td>  <td>1.5698</td>  <td>-6.2492</td> <td>0.0000</td> <td>-12.8870</td> <td>-6.7334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(schtyp)[T.2]</th> <td>0.4057</td>   <td>0.4765</td>  <td>0.8514</td>  <td>0.3945</td>  <td>-0.5282</td> <td>1.3397</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>science</th>        <td>0.1564</td>   <td>0.0270</td>  <td>5.7807</td>  <td>0.0000</td>  <td>0.1033</td>  <td>0.2094</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                         Results: Logit\n",
       "=================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.237     \n",
       "Dependent Variable: math_high        AIC:              175.8643  \n",
       "Date:               2020-08-13 21:05 BIC:              185.7592  \n",
       "No. Observations:   200              Log-Likelihood:   -84.932   \n",
       "Df Model:           2                LL-Null:          -111.36   \n",
       "Df Residuals:       197              LLR p-value:      3.3473e-12\n",
       "Converged:          1.0000           Scale:            1.0000    \n",
       "No. Iterations:     7.0000                                       \n",
       "-----------------------------------------------------------------\n",
       "                  Coef.  Std.Err.    z    P>|z|   [0.025   0.975]\n",
       "-----------------------------------------------------------------\n",
       "Intercept        -9.8102   1.5698 -6.2492 0.0000 -12.8870 -6.7334\n",
       "C(schtyp)[T.2]    0.4057   0.4765  0.8514 0.3945  -0.5282  1.3397\n",
       "science           0.1564   0.0270  5.7807 0.0000   0.1033  0.2094\n",
       "=================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hsb2['schtyp'] = hsb2['schtyp'].replace({1: 'public', 2: 'private'})\n",
    "#ols_model = sm.ols(formula = \"write ~ read + C(ses, Treatment(reference = 'low'))\", data = hsb2)\n",
    "logit_model = sm.logit(formula = \"math_high ~ C(schtyp) + science\", data = hsb2)\n",
    "logit_model = logit_model.fit()\n",
    "hsb2['math_high'] = logit_model.predict(hsb2)\n",
    "logit_model.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print a summary of the model you fit. What are important information about the model worth reporting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are the model's parameters. Based on what you know about interpreting model parameters in linear regression, how would you interpret the model parameters for the logistic regression model? HINT: It's a little more relatable to try to interpret $e^{b_i}$ instead of $b_i$ itself, using `np.exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept         0.000055\n",
      "C(schtyp)[T.2]    1.500394\n",
      "science           1.169250\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(logit_model.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how $P(Y = 1)$ changes with the science score for different school types, we can simulate a data with combinations of science scores and school types and predict for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>science</th>\n",
       "      <th>schtyp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.010101</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.020202</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.030303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.040404</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     science  schtyp\n",
       "0  20.000000     1.0\n",
       "1  21.010101     1.0\n",
       "2  22.020202     1.0\n",
       "3  23.030303     1.0\n",
       "4  24.040404     1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(20, 120, 100) # generate 100 examples of science score between 20 and 120\n",
    "y = [1, 2]\n",
    "X = np.transpose([np.tile(x, len(y)), np.repeat(y, len(x))])\n",
    "sim_data = pd.DataFrame(X, columns = ['science', 'schtyp'])\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict $P(Y = 1)$ for the above data and add it as a new column called `prob_math_high`. Then replace school type values with their proper labels `{1: 'public', 2: 'private'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data['prob_math_high'] = logit_model.predict(sim_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a line plot of `P(Y = 1)` against `science`, color-coded by `schtyp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12abeda90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot('science', 'prob_math_high', data = sim_data, hue = 'schtyp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above predictions are what we call **soft predictions** because they are probabilities, as opposed to **hard predictions** which are `True, False` or `1, 0` predictions. To get the hard predictions, we first need to set a threshold $q \\in (0, 1)$ and predict\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\hat Y = \\begin{cases}\n",
    "             0  & \\text{if } P(Y = 1) < q \\\\\n",
    "             1  & \\text{if } P(Y = 1) \\ge q\n",
    "     \\end{cases} \\quad\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Therefore hard predictions depend on the choice of the threshold $q$. All subsequent measures that are calculated from hard predictions (TP, TN, FP, FN, precision and recall, etc.) also depend on $q$.\n",
    "\n",
    "- Use the model's `pred_table` method to get the **confusion matrix**. By default, this is based on $q = 0.50$ but change the value of $q$ to $0.30$. What is the precision and recall of this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the sensitivity and specificity of the test. Sensitivity and specificity are two measures that are used to evaluate a binary classifier. We define **sensitivity** as $\\frac{TP}{FN + TP}$ and **specificity** as $\\frac{TN}{FP + TN}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the below cell the relationship between the $q$ and sensitivity or specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'q': np.linspace(0, 1, num = 50)})\n",
    "res['precision'] = 0\n",
    "res['recall'] = 0\n",
    "epsilon = 0.0001\n",
    "\n",
    "for index, row in res.iterrows():\n",
    "    cm = logit_model.pred_table(threshold = row['q'])\n",
    "    res.loc[index, 'specificity'] = cm[0, 0] / (cm[0, :].sum() + epsilon)\n",
    "    res.loc[index, 'sensitivity'] = cm[1, 1] / (cm[1, :].sum() + epsilon)\n",
    "\n",
    "sns.lineplot('q', 'sensitivity', data = res, color = 'blue')\n",
    "sns.lineplot('q', 'specificity', data = res, color = 'red')\n",
    "plt.legend(['senstivity', 'specificity']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain why sensitivity drops as $q$ increases while specificity increases.\n",
    "\n",
    "There are other ways to evaluate binary classification models, such as precision and recall. Sensitivity and specificity is more common in the medical field while precision and recall come from information retrieval. Recall and sensitivity have the same definition, but precision and specificity are not the same thing and should not be confused.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We learned how to extend linear regression for dealing with various situations. As you can see, linear regression gives us a very flexible building block. For this reason, many other algorithms build on the same ideas introduced in linear regression. We also saw how the coefficients in linear regression lend themselves to a simple interpretation, and how more complex models make this interpretation harder to maintain."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "727.717px",
    "left": "0px",
    "top": "111.483px",
    "width": "220.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
